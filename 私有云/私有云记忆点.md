数据库调优

```
修改/etc/my.cnf 文件，完成下列要求：
 1.设置数据库支持大小写；
 2.设置数据库缓存 innodb 表的索引，数据，插入数据时的缓冲为 4G；
 3.设置数据库的 log buffer 为 64MB；
 4.设置数据库的 redo log 大小为 256MB；
 5.设置数据库的 redo log 文件组为 2
 
#可以快速定位参数
mysql -uroot -p000000
MariaDB [(none)]>  show variables like 'innodb_log%';

vim /etc/my.cnf
#数据库支持大小写
lower_case_table_names =1
#数据库缓存
innodb_buffer_pool_size = 4G
#数据库的log buffer即redo日志缓冲
innodb_log_buffer_size = 64MB
#设置数据库的redo log即redo日志大小
innodb_log_file_size = 256MB
#数据库的redo log文件组即redo日志的个数配置
innodb_log_files_in_group = 2
```

OpenStack平台调度策略优化

```
安装完成后，请修改 nova 相关配置文件，解决因等待时间过长而导致虚拟机启动超时从而获取不 到 IP 地址而报错失败的问题

cat /etc/nova/nova.conf |grep vif_plugging_is_fatal
vif_plugging_is_fatal=false

systemctl restart openstack-nova*
```

解决了 ALL-in-one 快照在其他云平台 Dashboard 不能访问

```
 在controller节点上使用iaas-install-dashboad.sh脚本安装dashboad服务。安装完成后， 将 Dashboard 中的 Djingo 数据修改为存储在文件中（此种修改解决了 ALL-in-one 快照在其他云平台 Dashboard 不能访问的问题）。完成后提交控制节点的用户名、密码和 IP 地址到 答题框
 

#根据题意找线索，是关于django的数据，这个单词他写错了，就检索出来所有关于他的配置
#会发现有一行
#SESSION_ENGINE = 'django.contrib.sessions.backends.cache'存在cache里，改一改就行。

cat /etc/openstack-dashboard/local_settings |grep django
SESSION_ENGINE = 'django.contrib.sessions.backends.file'

#重启服务生效配置
systemctl restart httpd
```

使用提供的OpenStack私有云平台，修改相关配置文件，启用-device virtio-net-pci in kvm

```sh
修改 /etc/nova/nova.conf
--libvirt_use_virtio_for_bridges=true
```

Barbican创建密钥

```
openstack secret store --name secret01 --payload secretkey 
```

OpenStack 平台内存优化

```
搭建完 OpenStack 平台后，关闭系统的内存共享，打开透明大页。
 
#关闭系统的内存共享
echo 'never' > /sys/kernel/mm/transparent_hugepage/defrag
#验证
cat /sys/kernel/mm/transparent_hugepage/defrag
always madvise [never]
```

OpenStack平台I/O优化

```
当前全是用的是SSD硬盘，那么none算法更合适
[root@test ~]# echo none > /sys/block/vda/queue/scheduler
```

Linux系统文件句柄优化

```
Linux 服务器大并发时，往往需要预先调优 Linux 参数。默认情况下，Linux 最大文件 句柄数为 1024 个。当你的服务器在大并发达到极限时，就会报出“too many open files”。 创建一台云主机，修改相关配置，将控制节点的最大文件句柄数永久修改为 65535。

#临时生效
ulimit -n 65535

#永久生效
cat >> /etc/security/limits.conf <<EOF
* soft nofile 65535
* hard nofile 65535
EOF
```

Linux 系统调优-防止 SYN 攻击

```
 修改 controller 节点的相关配置文件，开启 SYN cookie，防止 SYN 洪水攻击。
 
 #默认的SYN相关配置
sysctl -a | grep _syn
	net.ipv4.tcp_max_syn_backlog = 512
	net.ipv4.tcp_syn_retries = 6
	net.ipv4.tcp_synack_retries = 5
	net.ipv4.tcp_syncookies = 1

#参数释义
tcp_max_syn_backlog是SYN队列的长度，加大SYN队列长度可以容纳更多等待连接的网络连接数。 tcp_syncookies是一个开关，是否打开SYN Cookie 功能，该功能可以防止部分SYN攻击。 tcp_synack_retries和tcp_syn_retries定义SYN 的重试连接次数，将默认的参数减小来控制SYN连接次数的尽量少。

#虽然默认是开启的，但是需要把配置添加到如下文件中
echo 'net.ipv4.tcp_syncookies = 1' >> /etc/sysctl.conf 

#生效配置
sysctl -p

```

Openstack平台镜像优化（镜像压缩）

```
#查看文件大小
du -sh CentOS7.5-compress.qcow2

#对镜像进行压缩
qemu-img convert -c -O qcow2 CentOS7.5-compress.qcow2 CentOS7.5-compress2.qcow2

-c 压缩
-O qcow2 输出格式为 qcow2
CentOS7.5-compress.qcow2   被压缩的文件
CentOS7.5-compress2.qcow2 压缩完成后文件
```

OpenStack Nova清除缓存

```bash
在OpenStack平台的一台计算节点创建虚拟机，若是第一次在该节点创建次虚拟机，会先将镜像文件复制到该计算节点目录/var/lib/nova/instances/_base。长期下来，该目录会占用比较大的磁盘空间而要清理。可以通过修改nova的配置文件来自动清理该缓存目录，即在该节点没有使用某镜像启动的云主机，那么这个镜像在过一定的时间后会被自动删除。
remove_unused_base_images=true
```

OpenStack网络

```
openstack network create --provider-network-type vlan --provider-physical-network provider network-vlan --provider-segment 200

openstack subnet create  --network network-vlan  --allocation-pool start=192.168.200.100,end=192.168.200.200 --gateway 192.168.200.1 --subnet-range 192.168.200.0/24  subnet-vlan
```

OpenStack安全组

```
创建安全组
openstack security group create test
添加安全规则
在“defualt”安全组中添加一条策略，从入口方向放行所有ICMP规则
openstack security group rule create --protocol icmp --ingress  default
在“defualt”安全组中添加一条策略，从入口方向放行所有TCP规则
openstack security group rule create --protocol tcp --ingress  default
在“defualt”安全组中添加一条策略，从入口方向放行所有UDP规则
openstack security group rule create --protocol udp --ingress  default
```

OpenStack容器

```
查看对象存储服务状态
swift stat
创建容器
openstack  container create  swift-test   #或  swift post chinaskill
创建对象（也是向容器中上传文件的过程）
openstack object create swift-test test/anaconda-ks.cfg 
下载对象
openstack object save swift-test test/anaconda-ks.cfg
删除对象
openstack object delete swift-test test/anaconda-ks.cfg

使用命令上传至test容器中，进行分片存储，每个片段的大小为10M
swift upload test -S 10000000 cirros-0.3.4-x86_64-disk.img
```

将提供的OpenStack云平台的安全策略从http优化至https

```sh
yum install -y https mod_ssl

vi /etc/openstack-dashboard/local-settings

CSRE_COOKIE_SECURE=True    #取消注释
SESSLON_COOKIE_SECURE=True     #取消注释

USE_SSL=True  #添加
SESSION_COOKIE_HTTPONLY=True    #取消注释

重启httpd memcached服务
```

在提供的OpenStack平台上，通过修改相关参数对openstack平台进行调优操作，相应的调优操作有：

（1）设置内存超售比例为1.5倍；

（2）设置nova服务心跳检查时间为120秒。

（3）预留2048mb内存，这部分内存不能被虚拟机使用

（4）预留10240mb磁盘，这部分磁盘不能被虚拟机使用

```
vi /etc/nova/nova.conf
ram_allocation_ratio=1.5
service_down_time=120

reserved_host_memory_mb=2048
reserved_host_disk_mb=10240
systemctl restart *nova*
```

在提供的OpenStack平台上，通过修改相关参数对openstack平台进行调优操作，相应的调优操作有：

（1）预留前2个物理CPU，把后面的所有CPU分配给虚拟机使用（假设vcpu为16个）；

（2）设置cpu超售比例为4倍；

```
vi /etc/nova/nova.conf
vcpu_pin_set = 2-15
cpu_allocation_ratio = 4.0

systemctl restart *nova*
```

对mencached服务进行操作使memcached的缓存由64MB变为256MB

```
vi /etc/sysconfig/memcached
PORT="11211"
USER="memcached"
MAXCONN="1024"
CACHESIZE="256"
OPTIONS="-l 127.0.0.1,::1,controller"

systemctl restart memcached
```

开放镜像权限

```
先将镜像共享给A租户
glance member-create 镜像id A租户id【projectA】
共享之后，镜像的状态是pending状态，还需要激活镜像
glance member-update  镜像id A租户id【projectA】 accepted
```

OpenStack对接堡垒机

```
安装依赖环境
yum install python2 -y
安装配置docker环境
cp -rf /opt/docker/* /usr/bin/
chmod 775 /usr/bin/docker*
cp -rf /opt/docker.service /etc/systemd/system/
systemctl daemon-reload
systemctl enable docker --now

加载jumpserver服务组件镜像
 sh load.sh
创建jumpser服务组件目录
mkdir -p /opt/jumpserver/{core,koko,lion,mysql,nginx,redis}
cp -rf /opt/config /opt/jumpserver/
生效环境变量static.env，使用所提供的脚本up.sh启动jumpserver服务
source /opt/static.env
sh up.sh

进入web创建远程连接用户root、创建资产（加入云主机）、资产授权（创建资产授权规则）
```

使用nfs作为glance镜像服务后端存储

```
配置该主机为nfs的server端，将该云主机中的/mnt/test目录进行共享（目录不存在可自行创建）。然后配置controller节点为nfs的client端，要求将/mnt/test目录作为glance后端存储的挂载目录
#nfs节点
yum isntall -y rpcbind nfs-utils
mkdir /mnt/test
vi /etc/exports
/mnt/exports *(rw,no_root_squash,sync)
exportfs -r
#controller
yum install -y nfs-utils
mount -t nfs 10.24.196.61:/mnt/test /var/lib/glance/images

df -Th
修改权限
cd /var/lib/glance
chown glance:glance images/
说明
sync：文件同时写入硬盘和内存
```

cinder作为glance镜像服务后端存储

```
在自行搭建的OpenStack平台中修改相关参数，使glance可以使用cinder作为后端存储，将镜像存储于cinder卷中。使用cirros-0.3.4-x86_64-disk.img文件创建cirros-image镜像存储于cirros-cinder卷中，通过cirros-image镜像使用cinder卷启动盘的方式进行创建虚拟机。

vi /etc/glance/glance-api.conf

stores = file, http, cinder
show_multiple_locations = True

/etc/cinder/cinder.conf
glance_api_version = 2     ##这个配置字段默认没有，在第300行那一片添加就行
allowed_direct_url_schemes = cinder
#以下字段在cinder配置文件里没有，直接在最后面添加就行
[lvm]
image_upload_use_internal_tenant = True

systemctl restart *glance* *cinder* 

创建镜像
openstack image create --container-format bare --disk-format raw --file /opt/openstack/images/cirros-0.3.4-x86_64-disk.img  cirros 
根据此镜像创建一个cinder卷
cinder create --image cirros --name cirros-cinder 1
根据刚才含有镜像的卷创建出cirros-image镜像
openstack image create --container-format bare --disk-format raw --volume cirros-cinder  cirros-image 
```

![image-20221110152858022](D:\疯狂内卷文件\云计算省赛准备\省赛记忆手册github\Provincial-competition-memory-handbook\私有云\私有云记忆点.assets\image-20221110152858022.png)

使用Swift对象存储服务，修改相应的配置文件，使对象存储Swift作为glance镜像服务的后端存储

```
vi /etc/glance/glance-api.conf

[glance_store]
default_store = swift
stores = glance.store.filesystem.Store,glance.store.swift.Store,glance.store.http.Store
swift_store_auth_address = http://controller:5000/v3.0
swift_store_endpoint_type = internalURL
swift_store_multi_tenant=True
swift_store_admin_tenants=service
swift_store_user=glance
swift_store_key=000000
swift_store_container=glance
swift_store_create_container_on_put=True
```

编写heat模板createvm.yml文件，模板作用为按照要求创建一个云主机

```yaml
# 查看资源类型
heat resource-type-list

# 查看可用于编排的模板版本（可在dashboard查看版本）
openstack orchestration template version list


heat_template_version: 2015-04-30
resources:
  server:
    type: OS::Nova::Server
    properties:
      name: "Test server"
      image: 7368e390-c311-4c8c-8767-2d0a2ffd4583
      flavor: m1.centos
      networks:
      - network: provider
```

编写heat模板文件，模板作用为按照要求创建一个云主机类型

```yaml
编写Heat模板create_flavor.yaml，创建名为“m1.flavor”、 ID 为 1234、内存为1024MB、硬盘为20GB、vcpu数量为 1的云主机类型。

cat create_flavor.yaml 
heat_template_version: 2015-04-30

resources:
  nova_flavor:
    type: OS::Nova::Flavor
    properties:
      name: m1.flavor
      disk: 20
      is_public: True
      ram: 1024
      vcpus: 2
      flavorid: 1234
outputs:	# 定义输出信息
  flavor_info:	# 输出信息的名称
    description: Get the information of virtual machine type	# 输出描述
    value: { get_attr: [ flavor, show ] }	# get_attr 从相应资源定义创建的实例在运行时解析其属性值进行输出

 
openstack stack create -t create_flavor.yaml test
```

编写heat模板文件，模板作用为按照要求创建网络

```yaml
简单写法
heat_template_version: 2015-04-30
resources:
  new_net:
    type: OS::Neutron::Net
    properties:
      admin_state_up: true
      name: Heat-Network
      shared: false

  new_subnet:
    type: OS::Neutron::Subnet
    properties:
      name: Heat-subnet
      allocation_pools:
      - start: 10.8.1.2
        end: 10.8.1.100
      network_id: { get_resource: new_net }
      cidr: 10.8.1.0/24
      dns_nameservers: [ "8.8.8.8", "8.8.4.4" ]
      ip_version: 4
      enable_dhcp: true
```

编写heat模板 文件，模板作用为按照要求创建一个卷(云硬盘)

```yaml
heat_template_version: 2015-04-30
resources:
  my_new_volume:
    type: OS::Cinder::Volume
    properties:
      size: 10
      name: mv
      volume_type: lvm
```

编写heat模板 文件，模板作用为按照要求创建一个用户

```yaml
创建名为heat-user的用户，属于admin项目，并赋予heat-user用户admin的权限，配置用户密码为123456
heat_template_version: 2015-04-30
resources:
  user:
    type: OS::Keystone::User
    properties:
      name: heat-user
      password: "123456"
      domain: demo
      default_project: admin
      roles: [{"role": admin, "project": admin}]
```

创建容器

```sh
heat_template_version: 2015-04-30
resources:
  the_resource:
    type: OS::Swift::Container
    properties:
      name: Heat-Swift
```

对cinder存储空间进行扩容操作，要求将cinder存储空间扩容10G

```
Cinder后端为lvm，这个扩容就是对LVM的扩容
# compute
pvcreate /dev/sdc       #分盘
vgextend cinder-volumes /dev/sdc   #加入
vgdisplay      #查看

# controller
cinder create --display-name cinder-volume-demo2 10     
#这是什么意思
```

manila共享文件系统服务

```
manila type-create default_share_type False
manila create NFS 2 --name share01
manila access-allow share01 ip 127.0.0.0/24 --access-level rw
manila access-list share01

manila show share01 | grep path | cut -d'|' -f3
mount -t nfs xxx,xxx,xxx,xxx:/var/lib/manila/mnt/share-xxxxxxx-xxxxxxxxx-xxxxxxxxx /mnt/

总结
创建类型、创建共享目录、开通共享权限、挂载
```

cloudkitty计费服务，处理来自不同监控指标后端的数据并进行计费规则创建。以达到费用核算目的。

```
# 设置规格为m1.small的云主机单价为1元
openstack rating hashmap service create instance_test
openstack rating hashmap field create cf8029bf-dc35-4e40-b8fd-5af4a4d25a30 flavor_name
openstack rating hashmap mapping create  --field-id xxxxxxxxx-xxxxxxxxx-xxxxxxx  -t flat --value  m1.small 1

# 镜像服务费用
openstack rating hashmap service create image_size_test
openstack rating hashmap mapping create -s xxxxxxxxx-xxxxxxxxx-xxxxxxx  -t flat 0.8
```

创建数据库插入数据

```
创建库test，并在库test中创建表company（表结构如(id int not null primary key,name varchar(50),addr varchar(255))所示），在表company中插入一条数据(1,"alibaba","china")

MariaDB [(none)]> create database test;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]> use test
Database changed
MariaDB [test]> create table company(id int not null primary key,name varchar(50),addr varchar(255));
Query OK, 0 rows affected (0.013 sec)

MariaDB [test]> insert into company values(1,"alibaba","china");
Query OK, 1 row affected (0.003 sec)
```

rabbitmq优化

```
通过修改limits.conf配置文件来修改RabbitMQ服务的最大连接数为10240

# 系统级别
vi /etc/sysctl.conf
  fs.file-max=10240

sysctl -p
# 用户级别
vi /etc/security/limits.conf
  openstack soft nofile 10240
  openstack hard nofile 10240

# rabbitmq的配置
vi /usr/lib/systemd/system/rabbitmq-server
  LimitNOFILE=10240

systemctl daemon-reload
systemctl restart rabbitmq-server

#查看 
rabbitmq status
```

swift分段存储

```
新建名为chinaskill的容器，将cirros-0.3.4-x86_64-disk.img镜像上传到chinaskill容器中，并设置分段存放，每一段大小为10M
swift post test      
swift stat test
swift upload test -S 10000000 cirros-0.3.4-x86_64-disk.img 
```

HAProxy+MariaDB   Galera   Cluster集群布置

```
三台机器
1. HAProxy
2.mariadb-server maeiadb
3.mariadb-server maeiadb
修改/etc/my.cnf/server.cnf
[galera]
wsrep_on=ON
wsrep_provider=/usr/lib64/galera/libgalera_smm.so
wsrep_cluster_address="gcomm://192.168.100.10,192.168.100.20"
binlog_fromat=row
default_storage_engine=Innodb
innodb_autoinc_lock_mode=2
bind_address=0.0.0.0

master节点 galera_new_cluster
slave节点 systemctl restart mariadb

HAProxy
修改/etc/haproxy/haproxy.cfg
lister mysql
  balance roundrobin
  mode tcp
  bind 192.168.100.30:3306
  server master 192.168.100.10:3306 check weight1
  server slave 192.168.100.20:3306 check weight1
  
listen stats
  bind 192.168.100.30:9000
  mode http
  stats enable
  stats uri /mysql
```

主从数据库

```
yum install -y mariadb mariadb-server
systemctl start mariadb &&  systemctl enable mariadb
# 主节点
vi /etc/my.cnf.d/server.cnf
[mysqld]
log_bin = mysql-bin
binlog_ignore_db = mysql
server_id = 1

systemctl restart mariadb

grant all privileges on *.* to root@'%' identified by '000000';
grant replication slave on *.* to 'user'@'%' identified by '000000';
flush privileges;

# 从节点
vi /etc/my.cnf.d/server.cnf
[mysqld]
log_bin = mysql-bin
binlog_ignore_db = mysql
server_id = 2

systemctl restart mariadb


change master to master_host='192.168.200.51',master_user='user',master_password='000000';
start slave;
show slave status\G;
```

读写分离（前提：主从数据库）

```
使用OpenStack私有云平台，创建三台云主机vm1、vm2和vm3，首先使用两台云主机完成MariaDB数据库的主从配置，接着根据提供的数据库中间件Mycat。完成Mycat读写分离数据库的配置安装（逻辑库名称使用“USERDB”，数据库密码使用000000）
yum install -y java* MariaDB-client

解压mycat到/usr/local/
修改conf/
将提供附件schema.xml替换,修改内容

删除server.xml最后并替换自己的密码和名称

启动/bin/mycat start            #记得授予数据库远程访问权限

查看 
mysql -uroot -pAbc@1234 -h 127.0.0.1 -P 8066
show @@datasource
```

RabbitMQ集群（三台）

```
yum install -y rabbitmq-server
修改主机名，写域名解析
启动服务后在/var/lib/rabbitmq下生成  .erlang.cookie
将三台cook换成相同的    wq!强制
重启改动cook的两台

如果传输的话# 传输完后会导致用户与用户组更改 需要改回
chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie


rabbitmqctl stop_app
停止后加入集群
rabbitmqctl join _cluster --ram rabbit@rabbitmq1    #一定要加域名解析
启动  rabbitmqctl start_app
查看  rabbitmqctl cluster_status


开启rabbitmq图形化面板  #端口15672

rabbitmq-plugins enable rabbitmq_management

systemctl restart rabbitmq-server
```

手动迁移云主机

```
1. 迁移虚拟机目录
/var/lib/nova/instances/中对应的虚拟机目录
2. 修改数据库文件
nova库中instances表对应虚拟机的host和node字段改成另一台主机名  
select * from instance \G;    查看uuid    
update instance set host='controller',node='controller' where uuid='XXX';
3. 重启opensatck-nova-compute
```

调整云主机大小配置

```
使用OpenStack私有云平台，使用centos7.9镜像，flavor使用1vcpu/2G内存/40G硬盘，创建云主机cscc_vm，假设在使用过程中，发现该云主机配置太低，需要调整，请修改相应配置，将dashboard界面上的云主机调整实例大小可以使用，将该云主机实例大小调整为2vcpu/4G内存/40G硬盘

#两个节点都改一下
vi /etc/nova/nova.conf 
[DEFAULT]
allow_resize_to_same_host = True 
scheduler_default_filters = RetryFilter,AvailabilityZoneFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter

systemctl restart openstack-nova*


openstack flavor create --disk 40 --ram 4096 --vcpus 2 centos1
openstack server resize --flavor centos1 --wait cscc_vm
```

将云主机打快照

```
将云主机打快照并保存到controller节点/root/cloudsave目录下，保存名字为csccvm.qcow2。最后使用qemu-img相关命令，将镜像的campat版本修改为0.10（该操作是为了适配某些低版本的云平台）
openstack server image create cirros-test --name cirrosimages

openstack image save --file /root/cloudsave/csccvm.qcow2   csccvm.qcow2

qemu-img info XXXXXX    查看镜像信息

qemu-img amend -f qcow2 -o compat=0.10 XXXX
```

磁盘阵列raid5

```
在OpenStack私有云平台，创建一台云主机，并创建一个40G大小的cinder块存储，将块存储连接到云主机，然后在云主机上对云硬盘进行操作。要求分出2个大小为10G的分区，使用这2个分区，创建名为/dev/md0、raid级别为0的磁盘阵列，最后将md0格式化为ext4格式并挂载至/mnt目录

cinder type-create lvm
cinder create --volume-type lvm --name block 40
openstack server add volume VM1 block

安装mdadm
yum install -y mdadm
分区
mdadm -C /dev/md1 -v -l 5 -n3 -x1 /dev/sdb{1,2,3,5}


查看 mdadm -D /dev/md1     #进度要到100%，不然0分
```

![image-20221107175235469](D:\疯狂内卷文件\云计算省赛准备\省赛记忆手册github\Provincial-competition-memory-handbook\私有云\私有云记忆点.assets\image-20221107175235469.png)

![image-20221107175757623](D:\疯狂内卷文件\云计算省赛准备\省赛记忆手册github\Provincial-competition-memory-handbook\私有云\私有云记忆点.assets\image-20221107175757623.png)

使用提供的OpenStack私有云平台，修改普通用户权限，使普通用户不能对镜像进行创建和删除操作

```
vi /etc/glance/policy.json
"add_image": " role:admin",
"delete_image": "role:admin",
```

数据库备份mysqldump

```
创建一台云主机，编写脚本，要求可以完成数据库的定期备份，并把数据库备份文件存放在/opt目录下
#!/bin/bash 
mysqldump -uroot -p000000 --all-databases > /opt/mysql.sql

定时命令
crontab -e
50 03 * * * cd /root/;sh mysql_backup.sh

crontab -l   查看

说明
 mysqldump 
Usage: mysqldump [OPTIONS] database [tables]
OR     mysqldump [OPTIONS] --databases [OPTIONS] DB1 [DB2 DB3...]
OR     mysqldump [OPTIONS] --all-databases [OPTIONS]
For more options, use mysqldump --help
```

调整虚拟机内存

```
使用OpenStack私有云平台，找到virsh中ID为10的云主机（若不存在请自行创建）。在云主机所在的物理节点，进入virsh交互式界面，使用virsh命令，将memory虚拟机的内存调整为5242880KB大小

virsh list --all
virsh shutdown instance-00000001
virsh edit instance-00000001
  <memory unit='KiB'>5242880</memory>
      <currentMemory unit='KiB'>5242880</currentMemory>

virsh start instance-00000001
```

kafka集群（依赖zookeeper集群）

```
zookeeper
安装java
重命名zoo.cfg
添加 server.1 = 192.168.100.10:2888:3888
    server.2 = 192.168.100.20:2888:3888
    server.3 = 192.168.100.30:2888:3888
创建数据文件夹  mkdir /tmp/zookeeper
将id写入myid   固定
echo "1"  >> /tmp/zookeeper/myid

启动服务

kafka集群
修改conf文件修改server.properties文件
broker.id=1
listeners=PLAINEXT://192.168.100.10:9092

启动服务
./kafka-server-start.sh -daemon ../config/server.properties
```

创建卷和逻辑卷

```
vgcreate vg1000 /dev/sdb1 /dev/sdb2  #创建卷组"vg1000"

使用lvcreate命令在卷组"vg1000"上创建一个200MB的逻辑卷。在命令行中输入下面的命令
lvcreate -L 200M vg1000    #创建大小为200M的逻辑卷
```

优化KVM的I/O调度算法

```
使用提供的OpenStack私有云平台，优化KVM的I/O调度算法，将默认的deadline修改为none模式
echo none > /sys/block/vda/queue/scheduler
```

redis一主二从

```
redis主从
修改配置文件
#主
#第一处修改
# bind 127.0.0.1                     //找到bind 127.0.0.1这行并注释掉
#第二处修改
protected-mode yes                   //修改前
protected-mode no                   //修改后，外部网络可以访问
#第三处修改
daemonize no                        //修改前
daemonize yes                       //修改后，开启守护进程
#第四处修改
# requirepass foobared                 //找到该行
requirepass "123456"                   //在下方添加设置访问密码
#第五处修改，设定主库密码与当前库密码同步，保证从库能够提升为主库
masterauth "123456"
#第六处修改，打开AOF持久化支持
appendonly yes

#从
#第一处修改
# bind 127.0.0.1                     //找到bind 127.0.0.1这行并注释掉
#第二处修改
protected-mode yes                   //修改前
protected-mode no                   //修改后，外部网络可以访问
#第三处修改
daemonize no                        //修改前
daemonize yes                       //修改后，开启守护进程
#第四处修改
# requirepass foobared                 //找到该行
requirepass "123456"                   //在下方添加设置访问密码
#第五处修改
# slaveof <masterip> <masterport>       //找到该行
slaveof 192.168.200.21 6379          //在下方添加访问的主节点IP与端口
#第六处修改
# masterauth <master-password>        //找到该行
masterauth "123456"                   //在下方添加访问主节点密码
#第七出修改，打开AOF持久化支持
appendonly yes

查看
redis-cli 
auth 123456
info

```

redis三哨兵

```
cp /etc/redis-sentinel.conf redis-sentinel.conf 
vi redis-sentinel.conf
protected-mode no
daemonize yes
sentinel monitor mymaster 192.168.200.51 6379 2
sentinel auth-pass mymaster 123456

启动   #先主再从
redis-sentinel redis-sentinel.conf 
```

云主机起不来

```
因为当前环境为本地PC环境的VMWare Workstation软件启动的虚拟机，所以在此openstack平台启动云主机，需要对openstack平台配置文件进行修改，修改nova服务配置文件，设置参数“virt_type=qemu”。

crudini --set /etc/nova/nova.conf libvirt virt_type  qemu
systemctl restart openstack-nova-compute
```

Vpn

```
在admin租户创建vpn连接，--peer-address为demo租户的路由route2网关地址100.0.0.22
[root@controller ~]# source /etc/keystone/admin-openrc.sh
[root@controller ~]# openstack vpn ike policy create ikepolicy1
[root@controller ~]# openstack vpn ipsec policy create ipsecpolicy1
[root@controller ~]# openstack vpn service create --router route1 --subnet net1 vpn1
[root@controller ~]# openstack vpn ipsec site connection create vpnconnectiona --vpnservice vpn1 --ikepolicy ikepolicy1  --ipsecpolicy ipsecpolicy1 --peer-address 100.0.0.22 --peer-id 100.0.0.22 --peer-cidr 100.0.2.0/24 --psk secret
```

