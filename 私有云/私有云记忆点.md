将提供的OpenStack云平台的安全策略从http优化至https

```sh
yum install -y https mod_ssl

vi /etc/openstack-dashboard/local-settings

CSRE_COOKIE_SECURE=True    #取消注释
SESSLON_COOKIE_SECURE=True     #取消注释

USE_SSL=True  #添加
SESSION_COOKIE_HTTPONLY=True    #取消注释

重启httpd memcached服务
```

在提供的OpenStack平台上，通过修改相关参数对openstack平台进行调优操作，相应的调优操作有：

（1）设置内存超售比例为1.5倍；

（2）设置nova服务心跳检查时间为120秒。

（3）预留2048mb内存，这部分内存不能被虚拟机使用

（4）预留10240mb磁盘，这部分磁盘不能被虚拟机使用

```
vi /etc/nova/nova.conf
ram_allocation_ratio=1.5
service_down_time=120

reserved_host_memory_mb=2048
reserved_host_disk_mb=10240
systemctl restart *nova*
```

在提供的OpenStack平台上，通过修改相关参数对openstack平台进行调优操作，相应的调优操作有：

（1）预留前2个物理CPU，把后面的所有CPU分配给虚拟机使用（假设vcpu为16个）；

（2）设置cpu超售比例为4倍；

```
vi /etc/nova/nova.conf
vcpu_pin_set = 2-15
cpu_allocation_ratio = 4.0

systemctl restart *nova*
```

对mencached服务进行操作使memcached的缓存由64MB变为256MB

```
vi /etc/sysconfig/memcached
PORT="11211"
USER="memcached"
MAXCONN="1024"
CACHESIZE="256"
OPTIONS="-l 127.0.0.1,::1,controller"

systemctl restart memcached
```

使用nfs作为glance镜像服务后端存储

```
配置该主机为nfs的server端，将该云主机中的/mnt/test目录进行共享（目录不存在可自行创建）。然后配置controller节点为nfs的client端，要求将/mnt/test目录作为glance后端存储的挂载目录
#nfs节点
yum isntall -y rpcbind nfs-utils
mkdir /mnt/test
vi /etc/exports
/mnt/exports *(rw,no_root_squash,sync)
exportfs -r
#controller
yum install -y nfs-utils
mount -t nfs 10.24.196.61:/mnt/test /var/lib/glance/images

df -Th
修改权限
cd /var/lib/glance
chown glance:glance images/
说明
sync：文件同时写入硬盘和内存
```

使用Swift对象存储服务，修改相应的配置文件，使对象存储Swift作为glance镜像服务的后端存储

```
vi /etc/glance/glance-api.conf

[glance_store]
default_store = swift
stores = glance.store.filesystem.Store,glance.store.swift.Store,glance.store.http.Store
swift_store_auth_address = http://controller:5000/v3.0
swift_store_endpoint_type = internalURL
swift_store_multi_tenant=True
swift_store_admin_tenants=service
swift_store_user=glance
swift_store_key=000000
swift_store_container=glance
swift_store_create_container_on_put=True
```

编写heat模板createvm.yml文件，模板作用为按照要求创建一个云主机

```yaml
heat_template_version: 2015-04-30
resources:
  server:
    type: OS::Nova::Server
    properties:
      name: "Test server"
      image: 7368e390-c311-4c8c-8767-2d0a2ffd4583
      flavor: m1.centos
      networks:
      - network: provider
```

编写heat模板文件，模板作用为按照要求创建一个云主机类型

```yaml
编写Heat模板create_flavor.yaml，创建名为“m1.flavor”、 ID 为 1234、内存为1024MB、硬盘为20GB、vcpu数量为 1的云主机类型。

cat create_flavor.yaml 
heat_template_version: 2015-04-30

resources:
  nova_flavor:
    type: OS::Nova::Flavor
    properties:
      name: m1.flavor
      disk: 20
      is_public: True
      ram: 1024
      vcpus: 2
      flavorid: 1234
 
openstack stack create -t create_flavor.yaml test
```

编写heat模板文件，模板作用为按照要求创建网络

```yaml
简单写法
heat_template_version: 2015-04-30
resources:
  new_net:
    type: OS::Neutron::Net
    properties:
      admin_state_up: true
      name: Heat-Network
      shared: false

  new_subnet:
    type: OS::Neutron::Subnet
    properties:
      name: Heat-subnet
      allocation_pools:
      - start: 10.8.1.2
        end: 10.8.1.100
      network_id: { get_resource: new_net }
      cidr: 10.8.1.0/24
      dns_nameservers: [ "8.8.8.8", "8.8.4.4" ]
      ip_version: 4
      enable_dhcp: true
```

编写heat模板 文件，模板作用为按照要求创建一个卷(云硬盘)

```yaml
heat_template_version: 2015-04-30
resources:
  my_new_volume:
    type: OS::Cinder::Volume
    properties:
      size: 10
      name: mv
      volume_type: lvm
```

编写heat模板 文件，模板作用为按照要求创建一个用户

```yaml
创建名为heat-user的用户，属于admin项目，并赋予heat-user用户admin的权限，配置用户密码为123456
heat_template_version: 2015-04-30
resources:
  user:
    type: OS::Keystone::User
    properties:
      name: heat-user
      password: "123456"
      domain: demo
      default_project: admin
      roles: [{"role": admin, "project": admin}]
```

对cinder存储空间进行扩容操作，要求将cinder存储空间扩容10G

```
# compute
pvcreate /dev/sdc       #分盘
vgextend cinder-volumes /dev/sdc   #加入
vgdisplay      #查看

# controller
cinder create --display-name cinder-volume-demo2 10     
#这是什么意思
```

manila共享文件系统服务

```
manila type-create default_share_type False
manila create NFS 2 --name share01
manila access-allow share01 ip 127.0.0.0/24 --access-level rw
manila access-list share01

manila show share01 | grep path | cut -d'|' -f3
mount -t nfs xxx,xxx,xxx,xxx:/var/lib/manila/mnt/share-xxxxxxx-xxxxxxxxx-xxxxxxxxx /mnt/

总结
创建类型、创建共享目录、开通共享权限、挂载
```

cloudkitty计费服务，处理来自不同监控指标后端的数据并进行计费规则创建。以达到费用核算目的。

```
# 设置规格为m1.small的云主机单价为1元
openstack rating hashmap service create instance_test
openstack rating hashmap field create cf8029bf-dc35-4e40-b8fd-5af4a4d25a30 flavor_name
openstack rating hashmap mapping create  --field-id xxxxxxxxx-xxxxxxxxx-xxxxxxx  -t flat --value  m1.small 1

# 镜像服务费用
openstack rating hashmap service create image_size_test
openstack rating hashmap mapping create -s xxxxxxxxx-xxxxxxxxx-xxxxxxx  -t flat 0.8
```

创建数据库插入数据

```
创建库test，并在库test中创建表company（表结构如(id int not null primary key,name varchar(50),addr varchar(255))所示），在表company中插入一条数据(1,"alibaba","china")

MariaDB [(none)]> create database test;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]> use test
Database changed
MariaDB [test]> create table company(id int not null primary key,name varchar(50),addr varchar(255));
Query OK, 0 rows affected (0.013 sec)

MariaDB [test]> insert into company values(1,"alibaba","china");
Query OK, 1 row affected (0.003 sec)
```

通过修改limits.conf配置文件来修改RabbitMQ服务的最大连接数为10240

```
# 系统级别
vi /etc/sysctl.conf
  fs.file-max=10240

# 用户级别
vi /etc/security/limits.conf
  openstack soft nofile 10240
  openstack hard nofile 10240

# rabbitmq的配置
vi /usr/lib/systemd/system/rabbitmq-server
  LimitNOFILE=10240

systemctl daemon-reload
systemctl restart rabbitmq-server
```

swift分段存储

```
新建名为chinaskill的容器，将cirros-0.3.4-x86_64-disk.img镜像上传到chinaskill容器中，并设置分段存放，每一段大小为10M
swift post test      
swift stat test
swift upload test -S 10000000 cirros-0.3.4-x86_64-disk.img 
```

HAProxy+MariaDB   Galera   Cluster集群布置

```
三台机器
1. HAProxy
2.mariadb-server maeiadb
3.mariadb-server maeiadb
修改/etc/my.cnf/server.cnf
[galera]
wsrep_on=ON
wsrep_provider=/usr/lib64/galera/libgalera_smm.so
wsrep_cluster_address="gcomm://192.168.100.10,192.168.100.20"
binlog_fromat=row
default_storage_engine=Innodb
innodb_autoinc_lock_mode=2
bind_address=0.0.0.0

master节点 galera_new_cluster
slave节点 systemctl restart mariadb

HAProxy
修改/etc/haproxy/haproxy.cfg
lister mysql
  balance roundrobin
  mode tcp
  bind 192.168.100.30:3306
  server master 192.168.100.10:3306 check weight1
  server slave 192.168.100.20:3306 check weight1
  
listen stats
  bind 192.168.100.30:9000
  mode http
  stats enable
  stats uri /mysql
```

主从数据库

```
yum install -y mariadb mariadb-server
systemctl start mariadb &&  systemctl enable mariadb
# 主节点
vi /etc/my.cnf.d/server.cnf
[mysqld]
log_bin = mysql-bin
binlog_ignore_db = mysql
server_id = 1

systemctl restart mariadb

grant all privileges on *.* to root@'%' identified by '000000';
grant replication slave on *.* to 'user'@'%' identified by '000000';
flush privileges;

# 从节点
vi /etc/my.cnf.d/server.cnf
[mysqld]
log_bin = mysql-bin
binlog_ignore_db = mysql
server_id = 2

systemctl restart mariadb


change master to master_host='192.168.200.51',master_user='user',master_password='000000';
start slave;
show slave status\G;
```

读写分离（前提：主从数据库）

```
使用OpenStack私有云平台，创建三台云主机vm1、vm2和vm3，首先使用两台云主机完成MariaDB数据库的主从配置，接着根据提供的数据库中间件Mycat。完成Mycat读写分离数据库的配置安装（逻辑库名称使用“USERDB”，数据库密码使用000000）
yum install -y java* MariaDB-client

解压mycat到/usr/local/
修改conf/
将提供附件schema.xml替换,修改内容

删除server.xml最后并替换自己的密码和名称

启动/bin/mycat start            #记得授予数据库远程访问权限

查看 
mysql -uroot -pAbc@1234 -h 127.0.0.1 -P 8066
show @@datasource
```

RabbitMQ集群（三台）

```
yum install -y rabbitmq-server
修改主机名，写域名解析
启动服务后在/var/lib/rabbitmq下生成  .erlang.cookie
将三台cook换成相同的    wq!强制
重启改动cook的两台

如果传输的话# 传输完后会导致用户与用户组更改 需要改回
chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie


rabbitmqctl stop_app
停止后加入集群
rabbitmqctl join _cluster --ram rabbit@rabbitmq1    #一定要加域名解析
启动  rabbitmqctl start_app
查看  rabbitmqctl cluster_status


开启rabbitmq图形化面板  #端口15672

rabbitmq-plugins enable rabbitmq_management

systemctl restart rabbitmq-server
```

手动迁移云主机

```
1. 迁移虚拟机目录
/var/lib/nova/instances/中对应的虚拟机目录
2. 修改数据库文件
nova库中instances表对应虚拟机的host和node字段改成另一台主机名  
select * from instance \G;    查看uuid    
update instance set host='controller',node='controller' where uuid='XXX';
3. 重启opensatck-nova-compute
```

调整云主机配置

```
使用OpenStack私有云平台，使用centos7.9镜像，flavor使用1vcpu/2G内存/40G硬盘，创建云主机cscc_vm，假设在使用过程中，发现该云主机配置太低，需要调整，请修改相应配置，将dashboard界面上的云主机调整实例大小可以使用，将该云主机实例大小调整为2vcpu/4G内存/40G硬盘

#两个节点都改一下
vi /etc/nova/nova.conf 
[DEFAULT]
allow_resize_to_same_host = True 
scheduler_default_filters = RetryFilter,AvailabilityZoneFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter

systemctl restart openstack-nova*


openstack flavor create --disk 40 --ram 4096 --vcpus 2 centos1
openstack server resize --flavor centos1 --wait cscc_vm
```

将云主机打快照

```
将云主机打快照并保存到controller节点/root/cloudsave目录下，保存名字为csccvm.qcow2。最后使用qemu-img相关命令，将镜像的campat版本修改为0.10（该操作是为了适配某些低版本的云平台）
openstack server image create cirros-test --name cirrosimages

openstack image save --file /root/cloudsave/csccvm.qcow2   csccvm.qcow2

qemu-img info XXXXXX    查看镜像信息

qemu-img amend -f qcow2 -o compat=0.10 XXXX
```

磁盘阵列raid5

```
在OpenStack私有云平台，创建一台云主机，并创建一个40G大小的cinder块存储，将块存储连接到云主机，然后在云主机上对云硬盘进行操作。要求分出2个大小为10G的分区，使用这2个分区，创建名为/dev/md0、raid级别为0的磁盘阵列，最后将md0格式化为ext4格式并挂载至/mnt目录

cinder type-create lvm
cinder create --volume-type lvm --name block 40
openstack server add volume VM1 block

安装mdadm
yum install -y mdadm
分区
mdadm -C /dev/md1 -v -l 5 -n3 -x1 /dev/sdb{1,2,3,5}


查看 mdadm -D /dev/md1     #进度要到100%，不然0分
```

![image-20221107175235469](D:\疯狂内卷文件\云计算省赛准备\省赛记忆手册github\Provincial-competition-memory-handbook\私有云\私有云记忆点.assets\image-20221107175235469.png)

![image-20221107175757623](D:\疯狂内卷文件\云计算省赛准备\省赛记忆手册github\Provincial-competition-memory-handbook\私有云\私有云记忆点.assets\image-20221107175757623.png)

使用提供的OpenStack私有云平台，修改普通用户权限，使普通用户不能对镜像进行创建和删除操作

```
vi /etc/glance/policy.json
"add_image": " role:admin",
"delete_image": "role:admin",
```

数据库备份mysqldump

```
创建一台云主机，编写脚本，要求可以完成数据库的定期备份，并把数据库备份文件存放在/opt目录下
#!/bin/bash 
mysqldump -uroot -p000000 --all-databases > /opt/mysql.sql

定时命令
crontab -e
50 03 * * * cd /root/;sh mysql_backup.sh

crontab -l   查看

说明
 mysqldump 
Usage: mysqldump [OPTIONS] database [tables]
OR     mysqldump [OPTIONS] --databases [OPTIONS] DB1 [DB2 DB3...]
OR     mysqldump [OPTIONS] --all-databases [OPTIONS]
For more options, use mysqldump --help
```

调整虚拟机内存

```
使用OpenStack私有云平台，找到virsh中ID为10的云主机（若不存在请自行创建）。在云主机所在的物理节点，进入virsh交互式界面，使用virsh命令，将memory虚拟机的内存调整为5242880KB大小

virsh list --all
virsh shutdown instance-00000001
virsh edit instance-00000001
  <memory unit='KiB'>5242880</memory>
      <currentMemory unit='KiB'>5242880</currentMemory>

virsh start instance-00000001
```

kafka集群（依赖zookeeper集群）

```
zookeeper
安装java
重命名zoo.cfg
添加 server.1 = 192.168.100.10:2888:3888
    server.2 = 192.168.100.20:2888:3888
    server.3 = 192.168.100.30:2888:3888
创建数据文件夹  mkdir /tmp/zookeeper
将id写入myid   固定
echo "1"  >> /tmp/zookeeper/myid

启动服务

kafka集群
修改conf文件修改server.properties文件
broker.id=1
listeners=PLAINEXT://192.168.100.10:9092

启动服务
./kafka-server-start.sh -daemon ../config/server.properties
```

创建卷和逻辑卷

```
vgcreate vg1000 /dev/sdb1 /dev/sdb2  #创建卷组"vg1000"

使用lvcreate命令在卷组"vg1000"上创建一个200MB的逻辑卷。在命令行中输入下面的命令
lvcreate -L 200M vg1000    #创建大小为200M的逻辑卷
```

优化KVM的I/O调度算法

```
使用提供的OpenStack私有云平台，优化KVM的I/O调度算法，将默认的deadline修改为none模式
echo none > /sys/block/vda/queue/scheduler
```

redis一主二从

```
redis主从
修改配置文件
#主
#第一处修改
# bind 127.0.0.1                     //找到bind 127.0.0.1这行并注释掉
#第二处修改
protected-mode yes                   //修改前
protected-mode no                   //修改后，外部网络可以访问
#第三处修改
daemonize no                        //修改前
daemonize yes                       //修改后，开启守护进程
#第四处修改
# requirepass foobared                 //找到该行
requirepass "123456"                   //在下方添加设置访问密码
#第五处修改，设定主库密码与当前库密码同步，保证从库能够提升为主库
masterauth "123456"
#第六处修改，打开AOF持久化支持
appendonly yes

#从
#第一处修改
# bind 127.0.0.1                     //找到bind 127.0.0.1这行并注释掉
#第二处修改
protected-mode yes                   //修改前
protected-mode no                   //修改后，外部网络可以访问
#第三处修改
daemonize no                        //修改前
daemonize yes                       //修改后，开启守护进程
#第四处修改
# requirepass foobared                 //找到该行
requirepass "123456"                   //在下方添加设置访问密码
#第五处修改
# slaveof <masterip> <masterport>       //找到该行
slaveof 192.168.200.21 6379          //在下方添加访问的主节点IP与端口
#第六处修改
# masterauth <master-password>        //找到该行
masterauth "123456"                   //在下方添加访问主节点密码
#第七出修改，打开AOF持久化支持
appendonly yes

查看
redis-cli 
auth 123456
info

```

redis三哨兵

```
cp /etc/redis-sentinel.conf redis-sentinel.conf 
vi redis-sentinel.conf
protected-mode no
daemonize yes
sentinel monitor mymaster 192.168.200.51 6379 2
sentinel auth-pass mymaster 123456

启动   #先主再从
redis-sentinel redis-sentinel.conf 
```













