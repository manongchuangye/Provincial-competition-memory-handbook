.使用RabbitMQ服务的相关命令创建用户chinaskill，密码为chinapd，并赋予该用户administrator权限

```
rabbitmqctl set_user_tags chinaskill administrator
```

使用 ovs-vswitchd 管理工具的相关命令查询计算节点的网桥列表信息

```
[root@compute ~]# ovs-vsctl   list-br
```

数据库调优

```bash
修改/etc/my.cnf 文件，完成下列要求：
 1.设置数据库支持大小写；
 2.设置数据库缓存 innodb 表的索引，数据，插入数据时的缓冲为 4G；
 3.设置数据库的 log buffer 为 64MB；
 4.设置数据库的 redo log 大小为 256MB；
 5.设置数据库的 redo log 文件组为 2
 
#可以快速定位参数
mysql -uroot -p000000
MariaDB [(none)]>  show variables like 'innodb_log%';

vim /etc/my.cnf
#数据库支持大小写
lower_case_table_names =1
#数据库缓存
innodb_buffer_pool_size = 4G
#数据库的log buffer即redo日志缓冲
innodb_log_buffer_size = 64MB
#设置数据库的redo log即redo日志大小
innodb_log_file_size = 256MB
#数据库的redo log文件组即redo日志的个数配置
innodb_log_files_in_group = 2

最好再进去mysql配置一下
set global
```

OpenStack平台调度策略优化

```
安装完成后，请修改 nova 相关配置文件，解决因等待时间过长而导致虚拟机启动超时从而获取不 到 IP 地址而报错失败的问题

cat /etc/nova/nova.conf |grep vif_plugging_is_fatal
vif_plugging_is_fatal=false

systemctl restart openstack-nova*
```

解决了 ALL-in-one 快照在其他云平台 Dashboard 不能访问

```
 在controller节点上使用iaas-install-dashboad.sh脚本安装dashboad服务。安装完成后， 将 Dashboard 中的 Djingo 数据修改为存储在文件中（此种修改解决了 ALL-in-one 快照在其他云平台 Dashboard 不能访问的问题）。完成后提交控制节点的用户名、密码和 IP 地址到 答题框
 

#根据题意找线索，是关于django的数据，这个单词他写错了，就检索出来所有关于他的配置
#会发现有一行
#SESSION_ENGINE = 'django.contrib.sessions.backends.cache'存在cache里，改一改就行。

cat /etc/openstack-dashboard/local_settings |grep django
SESSION_ENGINE = 'django.contrib.sessions.backends.file'

#重启服务生效配置
systemctl restart httpd
```

使用提供的OpenStack私有云平台，修改相关配置文件，启用-device virtio-net-pci in kvm

```sh
修改 /etc/nova/nova.conf
--libvirt_use_virtio_for_bridges=true
```

修改glance响应最大返回项数

```shell
glance服务安装完毕后，修改glance响应最大返回项数，该参数默认设置过短，可能导致响应数据被截断，镜像上传失败，修改该参数为1000。
vi /etc/glance/glance-api.conf 
	api_limit_max = 1000  # 与 nova 中的配置 osapi_max_limit 意义相同
	limit_param_default = 1000#一个响应中最大返回项数，可以在请求参数中指定，默认是25，如果设置过短，可能导致响应数据被截断

openstack-service restart


#### 以下为说明，不需要做
#vi /etc/nova/nova.conf
#	osapi_max_limit = 1000  #nova-api-os-compute api 的最大返回数据长度限制，如果设置过短，会导致部分响应数据被截断。
```

修改云平台中默认每个tenant的实例配额为20个

```bash
在controller节点和compute节点上分别使用iaas-install-nova -controller.sh脚本、iaas-install-nova-compute.sh脚本安装Nova 服务。安装完成后，修改云平台中默认每个tenant的实例配额为20个。

nova quota-class-update default --instances 20

nova quota-defaults
```

Barbican创建密钥

```
openstack secret store --name secret01 --payload secretkey 
```

OpenStack 平台内存优化

```bash
搭建完 OpenStack 平台后，关闭系统的内存共享，打开透明大页。
 
#关闭系统的内存共享
echo 'never' > /sys/kernel/mm/transparent_hugepage/defrag
#验证
cat /sys/kernel/mm/transparent_hugepage/defrag
always madvise [never]

# 查看
ipcs -sa
	...
	------ Shared Memory Segments --------
	key        shmid      owner      perms      bytes      nattch     status      
	0x50146cf0 4          root       600        1000       26     
	...

ipcrm -m shmid
```

```bash
#查看是否开启透明大页
cat /sys/kernel/mm/redhat_transparent_hugepage/enabled
#关闭透明大页
echo never > /sys/kernel/mm/redhat_transparent_hugepage/enabled  
cat /sys/kernel/mm/redhat_transparent_hugepage/enabled

#关闭透明大页后AnonHugePages应该为0
[root@db04 ~]#  grep AnonHugePages /proc/meminfo
AnonHugePages:    339968 kB

[root@db04 ~]#  grep AnonHugePages /proc/meminfo
AnonHugePages:         0 kB    -->重启服务器后透明大页为0

#永远关闭透明大页
cp /etc/grub.conf /etc/grub.conf.bak
vi /etc/grub.conf
#在kernel /vmlinuz-2.6.32-642.el6.x86_64 ro root=/dev/mapper/vg_dbd1-lv_root nomodeset rd_NO_LUKS rd_NO_MD rd_LVM_LV=vg_dbd1/lv_swap crashkernel=auto LANG=zh_CN.UTF-8 rd_LVM_LV=vg_dbd1/lv_root  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet
#后面添加
transparent_hugepage=never

-----linux 7
cat /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/enabled  
#关闭透明大页
cp /etc/default/grub /etc/default/grub.bak
vi /etc/default/grub
#在GRUB_CMDLINE_LINUX="crashkernel=auto rd.lvm.lv=cl/root rd.lvm.lv=cl/swap rhgb quiet"后添加
transparent_hugepage=never

执行生效命令 
grub2-mkconfig -o /boot/grub2/grub.cfg
后reboot服务器
检查是否生效
cat /proc/cmdline
grep AnonHugePages /proc/meminfo


2.开启大页内存
linux内核2.6以及以后的内核版本都具备HugePages特性

使用大页的好处
(1)页表数量减少
(2)sga不会存在内存交换

设置oracle(grid)用户的memlock
/etc/security/limits.conf
oracle soft memlock unlimited
oracle hard memlock unlimited

vi /etc/sysctl.conf
vm.nr_hugepages = num*2M > SGA
vm.nr_hugepages = 8602
#sysctl -p 

4.oracle启用ASMM ,停用AMM
memory_max_target / memory_target 设置为0
sga_max_size / sga_target / pga_aggregate_target 手动设置
sga_max_size=RPM*56%
pga_aggregate_target=RPM*14%


5.关闭AMM
alter system set memory_max_target=0 scope=spfile;
alter system set memory_target=0 scope=spfile;

alter system set sga_max_size=XXXM scope=spfile;
alter system set sga_target=XXXM scope=spfile;
alter system set pga_aggregate_target=XXXM scope=spfile;

#重启数据库
alter system checkpoint;
alter system switch logfile;
shut immediate
create pfile from spfile;
vi $ORACLE_HOME/dbs/initSID.orae
去掉memory_max_target、memory_target参数
create spfile from pfile;
startup 
```

OpenStack平台I/O优化

```
当前全是用的是SSD硬盘，那么none算法更合适
[root@test ~]# echo none > /sys/block/vda/queue/scheduler
```

Linux系统文件句柄优化

```
Linux 服务器大并发时，往往需要预先调优 Linux 参数。默认情况下，Linux 最大文件 句柄数为 1024 个。当你的服务器在大并发达到极限时，就会报出“too many open files”。 创建一台云主机，修改相关配置，将控制节点的最大文件句柄数永久修改为 65535。

#临时生效
ulimit -n 65535

#永久生效
cat >> /etc/security/limits.conf <<EOF
* soft nofile 65535
* hard nofile 65535
EOF
```

Linux 系统调优-防止 SYN 攻击

```
 修改 controller 节点的相关配置文件，开启 SYN cookie，防止 SYN 洪水攻击。
 
 #默认的SYN相关配置
sysctl -a | grep _syn
	net.ipv4.tcp_max_syn_backlog = 512
	net.ipv4.tcp_syn_retries = 6
	net.ipv4.tcp_synack_retries = 5
	net.ipv4.tcp_syncookies = 1

#参数释义
tcp_max_syn_backlog是SYN队列的长度，加大SYN队列长度可以容纳更多等待连接的网络连接数。 tcp_syncookies是一个开关，是否打开SYN Cookie 功能，该功能可以防止部分SYN攻击。 tcp_synack_retries和tcp_syn_retries定义SYN 的重试连接次数，将默认的参数减小来控制SYN连接次数的尽量少。

#虽然默认是开启的，但是需要把配置添加到如下文件中
echo 'net.ipv4.tcp_syncookies = 1' >> /etc/sysctl.conf 

#生效配置
sysctl -p

```

内核优化

```bash
开启SYN Cookies;	允许将TIME-WAIT sockets用于新的TCP连接；
开启TCP连接中的TIME-WAIT sockets快速回收；修改系统默认的时间为30s；

sysctl -a | grep 

## vi /etc/sysctl.conf
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_fin_timeout = 30
```

Openstack平台镜像优化（镜像压缩）

```
#查看文件大小
du -sh CentOS7.5-compress.qcow2

#对镜像进行压缩
qemu-img convert -c -O qcow2 CentOS7.5-compress.qcow2 CentOS7.5-compress2.qcow2

-c 压缩
-O qcow2 输出格式为 qcow2
CentOS7.5-compress.qcow2   被压缩的文件
CentOS7.5-compress2.qcow2 压缩完成后文件
```

OpenStack Nova清除缓存

```bash
在OpenStack平台的一台计算节点创建虚拟机，若是第一次在该节点创建次虚拟机，会先将镜像文件复制到该计算节点目录/var/lib/nova/instances/_base。长期下来，该目录会占用比较大的磁盘空间而要清理。可以通过修改nova的配置文件来自动清理该缓存目录，即在该节点没有使用某镜像启动的云主机，那么这个镜像在过一定的时间后会被自动删除。
remove_unused_base_images=true
```

OpenStack网络

```
openstack network create --provider-network-type vlan --provider-physical-network provider network-vlan --provider-segment 200

openstack subnet create  --network network-vlan  --allocation-pool start=192.168.200.100,end=192.168.200.200 --gateway 192.168.200.1 --subnet-range 192.168.200.0/24  subnet-vlan
```

OpenStack安全组

```
创建安全组
openstack security group create test
添加安全规则
在“defualt”安全组中添加一条策略，从入口方向放行所有ICMP规则
openstack security group rule create --protocol icmp --ingress  default
在“defualt”安全组中添加一条策略，从入口方向放行所有TCP规则
openstack security group rule create --protocol tcp --ingress  default
在“defualt”安全组中添加一条策略，从入口方向放行所有UDP规则
openstack security group rule create --protocol udp --ingress  default
```

OpenStack容器

```
查看对象存储服务状态
swift stat
创建容器
openstack  container create  swift-test   #或  swift post chinaskill
创建对象（也是向容器中上传文件的过程）
openstack object create swift-test test/anaconda-ks.cfg 
下载对象
openstack object save swift-test test/anaconda-ks.cfg
删除对象
openstack object delete swift-test test/anaconda-ks.cfg

使用命令上传至test容器中，进行分片存储，每个片段的大小为10M
swift upload test -S 10000000 cirros-0.3.4-x86_64-disk.img
```

将提供的OpenStack云平台的安全策略从http优化至https

```sh
yum install -y https mod_ssl

vi /etc/openstack-dashboard/local-settings

CSRE_COOKIE_SECURE=True    #取消注释
SESSLON_COOKIE_SECURE=True     #取消注释

USE_SSL=True  #添加
SESSION_COOKIE_HTTPONLY=True    #取消注释

重启httpd memcached服务
```

在提供的OpenStack平台上，通过修改相关参数对openstack平台进行调优操作，相应的调优操作有：

（1）设置内存超售比例为1.5倍；

（2）设置nova服务心跳检查时间为120秒。

（3）预留2048mb内存，这部分内存不能被虚拟机使用

（4）预留10240mb磁盘，这部分磁盘不能被虚拟机使用

```
vi /etc/nova/nova.conf
ram_allocation_ratio=1.5
service_down_time=120

reserved_host_memory_mb=2048
reserved_host_disk_mb=10240
systemctl restart *nova*
```

在提供的OpenStack平台上，通过修改相关参数对openstack平台进行调优操作，相应的调优操作有：

（1）预留前2个物理CPU，把后面的所有CPU分配给虚拟机使用（假设vcpu为16个）；

（2）设置cpu超售比例为4倍；

```
vi /etc/nova/nova.conf
vcpu_pin_set = 2-15
cpu_allocation_ratio = 4.0

systemctl restart *nova*
```

对mencached服务进行操作使memcached的缓存由64MB变为256MB

```
vi /etc/sysconfig/memcached
PORT="11211"
USER="memcached"
MAXCONN="1024"
CACHESIZE="256"
OPTIONS="-l 127.0.0.1,::1,controller"

systemctl restart memcached
```

开放镜像权限

```
先将镜像共享给A租户
glance member-create 镜像id A租户id【projectA】
共享之后，镜像的状态是pending状态，还需要激活镜像
glance member-update  镜像id A租户id【projectA】 accepted
```

OpenStack对接堡垒机

```
安装依赖环境
yum install python2 -y
安装配置docker环境
cp -rf /opt/docker/* /usr/bin/
chmod 775 /usr/bin/docker*
cp -rf /opt/docker.service /etc/systemd/system/
systemctl daemon-reload
systemctl enable docker --now

加载jumpserver服务组件镜像
 sh load.sh
创建jumpser服务组件目录
mkdir -p /opt/jumpserver/{core,koko,lion,mysql,nginx,redis}
cp -rf /opt/config /opt/jumpserver/
生效环境变量static.env，使用所提供的脚本up.sh启动jumpserver服务
source /opt/static.env
sh up.sh

进入web创建远程连接用户root、创建资产（加入云主机）、资产授权（创建资产授权规则）
```

使用nfs作为glance镜像服务后端存储

```
配置该主机为nfs的server端，将该云主机中的/mnt/test目录进行共享（目录不存在可自行创建）。然后配置controller节点为nfs的client端，要求将/mnt/test目录作为glance后端存储的挂载目录
#nfs节点
yum isntall -y rpcbind nfs-utils
mkdir /mnt/test
vi /etc/exports
/mnt/exports *(rw,no_root_squash,sync)
exportfs -r
#controller
yum install -y nfs-utils
mount -t nfs 10.24.196.61:/mnt/test /var/lib/glance/images

df -Th
修改权限
cd /var/lib/glance
chown glance:glance images/
说明
sync：文件同时写入硬盘和内存
```

cinder作为glance镜像服务后端存储

```bash
在自行搭建的OpenStack平台中修改相关参数，使glance可以使用cinder作为后端存储，将镜像存储于cinder卷中。使用cirros-0.3.4-x86_64-disk.img文件创建cirros-image镜像存储于cirros-cinder卷中，通过cirros-image镜像使用cinder卷启动盘的方式进行创建虚拟机。

vi /etc/glance/glance-api.conf

stores = file, http, cinder
show_multiple_locations = True

/etc/cinder/cinder.conf
glance_api_version = 2     ##这个配置字段默认没有，在第300行那一片添加就行
allowed_direct_url_schemes = cinder
#以下字段在cinder配置文件里没有，直接在最后面添加就行
[lvm]
image_upload_use_internal_tenant = True

systemctl restart *glance* *cinder* 

创建镜像
openstack image create --container-format bare --disk-format raw --file /opt/openstack/images/cirros-0.3.4-x86_64-disk.img  cirros 
根据此镜像创建一个cinder卷        
cinder create --image cirros --name cirros-cinder 1
根据刚才含有镜像的卷创建出cirros-image镜像
openstack image create --container-format bare --disk-format raw --volume cirros-cinder  cirros-image 
```

![image-20221110152858022](D:\疯狂内卷文件\云计算省赛准备\省赛记忆手册github\Provincial-competition-memory-handbook\私有云\私有云记忆点.assets\image-20221110152858022.png)

对象存储Swift作为glance镜像服务的后端存储

```
使用OpenStack私有云平台，使用Swift对象存储服务，修改相应的配置文件，使对象存储Swift作为glance镜像服务的后端存储，使默认上传的镜像会在swift中创建chinaskill_glance容器。配置完成后上传镜像测试。
vi /etc/glance/glance-api.conf
将原有的注释掉
[glance_store]
#stores = file,http
#default_store = file
#filesystem_store_datadir = /var/lib/glance/images/
修改为
default_store = swift
stores=glance.store.filesystem.Stroe,glance.store.http.Store,glance.store.swift.Store
swift_store_auth_address=http://controller:5000/v3.0
swift_store_endpoint_type=internalURL
swift_store_multi_tenant=True
swift_store_admin_tenants=service
swift_store_user=glance
swift_store_key=000000
swift_store_container=glance
swift_store_create_container_on_put=True
```

编写heat模板createvm.yml文件，模板作用为按照要求创建一个云主机

```yaml
# 查看资源类型
heat resource-type-list

# 查看可用于编排的模板版本（可在dashboard查看版本）
openstack orchestration template version list


heat_template_version: 2015-04-30
resources:
  server:
    type: OS::Nova::Server
    properties:
      name: "Test server"
      image: 7368e390-c311-4c8c-8767-2d0a2ffd4583
      flavor: m1.centos
      networks:
      - network: provider
```

编写heat模板文件，模板作用为按照要求创建一个云主机类型

```yaml
编写Heat模板create_flavor.yaml，创建名为“m1.flavor”、 ID 为 1234、内存为1024MB、硬盘为20GB、vcpu数量为 1的云主机类型。

cat create_flavor.yaml 
heat_template_version: 2015-04-30

resources:
  nova_flavor:
    type: OS::Nova::Flavor
    properties:
      name: m1.flavor
      disk: 20
      is_public: True
      ram: 1024
      vcpus: 2
      flavorid: 1234
outputs:	# 定义输出信息
  flavor_info:	# 输出信息的名称
    description: Get the information of virtual machine type	# 输出描述
    value: { get_attr: [ flavor, show ] }	# get_attr 从相应资源定义创建的实例在运行时解析其属性值进行输出

 
openstack stack create -t create_flavor.yaml test
```

编写heat模板文件，模板作用为按照要求创建网络

```yaml
简单写法
heat_template_version: 2015-04-30
resources:
  new_net:
    type: OS::Neutron::Net
    properties:
      admin_state_up: true
      name: Heat-Network
      shared: false

  new_subnet:
    type: OS::Neutron::Subnet
    properties:
      name: Heat-subnet
      allocation_pools:
      - start: 10.8.1.2
        end: 10.8.1.100
      network_id: { get_resource: new_net }
      cidr: 10.8.1.0/24
      dns_nameservers: [ "8.8.8.8", "8.8.4.4" ]
      ip_version: 4
      enable_dhcp: true
```

编写heat模板 文件，模板作用为按照要求创建一个卷(云硬盘)

```yaml
heat_template_version: 2015-04-30
resources:
  my_new_volume:
    type: OS::Cinder::Volume
    properties:
      size: 10
      name: mv
      volume_type: lvm
```

编写heat模板 文件，模板作用为按照要求创建一个用户

```yaml
创建名为heat-user的用户，属于admin项目，并赋予heat-user用户admin的权限，配置用户密码为123456
heat_template_version: 2015-04-30
resources:
  user:
    type: OS::Keystone::User
    properties:
      name: heat-user
      password: "123456"
      domain: demo
      default_project: admin
      roles: [{"role": admin, "project": admin}]
```

创建容器

```sh
heat_template_version: 2015-04-30
resources:
  the_resource:
    type: OS::Swift::Container
    properties:
      name: Heat-Swift
```

对cinder存储空间进行扩容操作，要求将cinder存储空间扩容10G

```
Cinder后端为lvm，这个扩容就是对LVM的扩容
# compute
pvcreate /dev/sdc       #分盘
vgextend cinder-volumes /dev/sdc   #加入
vgdisplay      #查看

# controller
cinder create --display-name cinder-volume-demo2 10     
#这是什么意思
```

manila共享文件系统服务

```
manila type-create default_share_type False
manila create NFS 2 --name share01
manila access-allow share01 ip 127.0.0.0/24 --access-level rw
manila access-list share01

manila show share01 | grep path | cut -d'|' -f3
mount -t nfs xxx,xxx,xxx,xxx:/var/lib/manila/mnt/share-xxxxxxx-xxxxxxxxx-xxxxxxxxx /mnt/

总结
创建类型、创建共享目录、开通共享权限、挂载
```

cloudkitty计费服务，处理来自不同监控指标后端的数据并进行计费规则创建。以达到费用核算目的。

```
#使用 iaas-install-cloudkitty.sh 脚本安装 cloudkitty 服务，安装完毕后，启用 hashmap 评级 模块，接着创建 volume_thresholds 组，创建服务匹配规则 volume.size，并设置每 GB 的价 格为 0.01。接下来对应大量数据设置应用折扣，在组 volume_thresholds 中创建阈值，设置 若超过 50GB 的阈值，应用 2%的折扣（0.98）。设置完成后提交控制节点的用户名、密码 和 IP 地址到答题框
#0.启用hashmap
[root@controller ~]# openstack rating module enable hashmap 

#1.创建hashmap service
[root@controller ~]# openstack rating  hashmap service create volume.size 
+--------+--------------------------------------+
| Name   | Service ID                           |
+--------+--------------------------------------+
| volume.size | 09da4a8b-b849-4715-a8e3-7cd12dfcf46e |
+--------+--------------------------------------+

#2.创建hashmap service group
[root@controller ~]# openstack rating hashmap group create  volume_thresholds 
openstack rating hashmap group create  volume_thresholds 
+-------------------+--------------------------------------+
| Name              | Group ID                             |
+-------------------+--------------------------------------+
| volume_thresholds | 8b3dfe73-5efb-46ab-a93b-dc9519063ed6 |
+-------------------+--------------------------------------+

#3.创建volume单价
[root@controller ~]# openstack rating hashmap mapping create   -s  09da4a8b-b849-4715-a8e3-7cd12dfcf46e -g 8b3dfe73-5efb-46ab-a93b-dc9519063ed6  -t flat  0.01  

#4.创建service rule
[root@controller ~]# openstack rating hashmap threshold create   -s  09da4a8b-b849-4715-a8e3-7cd12dfcf46e -g 8b3dfe73-5efb-46ab-a93b-dc9519063ed6  -t rate 50 0.98

# 设置规格为m1.small的云主机单价为1元
openstack rating hashmap service create instance_test
openstack rating hashmap field create cf8029bf-dc35-4e40-b8fd-5af4a4d25a30 flavor_name
openstack rating hashmap mapping create  --field-id xxxxxxxxx-xxxxxxxxx-xxxxxxx  -t flat --value  m1.small 1

# 镜像服务费用
openstack rating hashmap service create image_size_test
openstack rating hashmap mapping create -s xxxxxxxxx-xxxxxxxxx-xxxxxxx  -t flat 0.8
```

创建数据库插入数据

```
创建库test，并在库test中创建表company（表结构如(id int not null primary key,name varchar(50),addr varchar(255))所示），在表company中插入一条数据(1,"alibaba","china")

MariaDB [(none)]> create database test;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]> use test
Database changed
MariaDB [test]> create table company(id int not null primary key,name varchar(50),addr varchar(255));
Query OK, 0 rows affected (0.013 sec)

MariaDB [test]> insert into company values(1,"alibaba","china");
Query OK, 1 row affected (0.003 sec)
```

OpenStack参数调优（rabbitmq优化）

```
OpenStack各服务内部通信都是通过RPC来交互，各agent都需要去连接RabbitMQ；随着各服务agent增多，MQ的连接数会随之增多，最终可能会到达上限，成为瓶颈。使用自行搭建的OpenStack私有云平台，分别通过用户级别、系统级别、配置文件来设置RabbitMQ服务的最大连接数为10240

# 系统级别
vi /etc/sysctl.conf
  fs.file-max=10240

sysctl -p
# 用户级别
vi /etc/security/limits.conf
  openstack soft nofile 10240
  openstack hard nofile 10240

# rabbitmq的配置
vi /usr/lib/systemd/system/rabbitmq-server
  LimitNOFILE=10240

systemctl daemon-reload
systemctl restart rabbitmq-server

#查看 
rabbitmq status
```

Cinder数据加密

```bash
使用自行创建的OpenStack云计算平台，通过相关配置，开启Cinder块存储的数据加密功能，然后创建加密卷类型luks，并配置卷类型luks使用带有512位密钥，Cipher使用aes-xts-plain64，Control Location使用front-end，Provider使用nova.volume.encryptors.luks.LuksEncryptor，最后分别创建两个大小为1G的云硬盘，一个是普通云硬盘，另一个使用加密卷类型。


数据加密：
前提：按要求配置静态 fixed_key，使 cinder 和 nova 组件可以使用加密过的 Block Storage 卷服务，配置好之后，创建一个卷类型叫 luks，并把这个类型配置 为加密类型，cipher 使用“aes-xts-plain64”，key_size 使用“512”，control-location 使用“front-end”，Provider 使用“nova.volume.encryptors.luks.LuksEncryptor”。

openstack-config --set /etc/cinder/cinder.conf keymgr fixed_key HEX_KEY
systemctl restart openstack-cinder-volume
openstack-config --set /etc/nova/nova.conf keymgr  fixed_key HEX_KEY
openstack-service restart nova-compute
cinder type-create luks
cinder encryption-type-create --cipher aes-xts-plain64 --key_size 512 --control_location front-end luks nova.volume.encryptors.luks.LuksEncryptor

1.使用命令查看卷类型列表和加密卷类型列表。
cinder type-list
cinder encryption-type-list

2.使用命令创建两个卷，前者不加密，后者使用 luks 卷类型加密。然后查看卷列表。
cinder create --display-name volume1  --volume-type type1 20
cinder create --display-name volume2  --volume-type luks 20
openstack volume list

3.使用命令创建两个卷，前者不加密，后者使用 luks 卷类型加密。使用 nova 命令，创建一个云主机，镜像使用提供的 cirros 镜像，然后使用命令分别将 创建的两块云硬盘 attach 到云主机上，最后使用 cinder list 查看。
cinder create --display-name volume1  --volume-type type1 20
cinder create --display-name volume2  --volume-type luks 20
nova boot  --flavor docker --image "examimage"  --nic net-id=c5221f87-5f27-4fb0-9e31-08269c104676 vm-test1
nova volume-attach vm-test1 e36e5bd8-167b-466e-829a-6be337761847    （此ID为云硬盘ID）
```

swift分段存储

```
新建名为chinaskill的容器，将cirros-0.3.4-x86_64-disk.img镜像上传到chinaskill容器中，并设置分段存放，每一段大小为10M
swift post test      
swift stat test
swift upload test -S 10000000 cirros-0.3.4-x86_64-disk.img 
```

HAProxy+MariaDB   Galera   Cluster集群布置

```
三台机器
1. HAProxy
2.mariadb-server maeiadb
3.mariadb-server maeiadb
修改/etc/my.cnf/server.cnf
[galera]
wsrep_on=ON
wsrep_provider=/usr/lib64/galera/libgalera_smm.so
wsrep_cluster_address="gcomm://192.168.100.10,192.168.100.20"
binlog_fromat=row
default_storage_engine=Innodb
innodb_autoinc_lock_mode=2
bind_address=0.0.0.0

master节点 galera_new_cluster
slave节点 systemctl restart mariadb

HAProxy
修改/etc/haproxy/haproxy.cfg
lister mysql
  balance roundrobin
  mode tcp
  bind 192.168.100.30:3306
  server master 192.168.100.10:3306 check weight1
  server slave 192.168.100.20:3306 check weight1
  
listen stats
  bind 192.168.100.30:9000
  mode http
  stats enable
  stats uri /mysql
```

主从数据库

```
yum install -y mariadb mariadb-server
systemctl enable --now mariadb
# 主节点
vi /etc/my.cnf.d/server.cnf
[mysqld]
log_bin = mysql-bin               #必须要写，不然报错
binlog_ignore_db = mysql          # #不同步mysql 系统数据库，可以不写
server_id = 1                     #必须要写，一般写ip主机位

systemctl restart mariadb

MariaDB [(none)]> grant all privileges on *.* to root@'%' identified by '000000';
# 注释：如果你不想配置上面的host文件可以不使用主机名mysql2的形式，可以直接打IP地址，用户可以随意指定，只是一个用于连接的而已
MariaDB [(none)]> grant replication slave on *.* to 'user'@'%' identified by '000000';
flush privileges;

# 从节点
vi /etc/my.cnf.d/server.cnf
[mysqld]
log_bin = mysql-bin
binlog_ignore_db = mysql
server_id = 2

systemctl restart mariadb


change master to master_host='192.168.200.51',master_user='user',master_password='000000';
start slave;
show slave status\G;
```

报错（未写log_bin = mysql-bin，只在主节点上写就可以了）

```bash
MariaDB [(none)]>  SHOW SLAVE STATUS \G;
*************************** 1. row ***************************
               Slave_IO_State: 
                  Master_Host: db1
                  Master_User: user
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: 
          Read_Master_Log_Pos: 4
               Relay_Log_File: mariadb-relay-bin.000004
                Relay_Log_Pos: 4
        Relay_Master_Log_File: 
             Slave_IO_Running: No
            Slave_SQL_Running: Yes
              Replicate_Do_DB: 
          Replicate_Ignore_DB: 
           Replicate_Do_Table: 
       Replicate_Ignore_Table: 
      Replicate_Wild_Do_Table: 
  Replicate_Wild_Ignore_Table: 
                   Last_Errno: 0
                   Last_Error: 
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 245
              Relay_Log_Space: 245
              Until_Condition: None
               Until_Log_File: 
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File: 
           Master_SSL_CA_Path: 
              Master_SSL_Cert: 
            Master_SSL_Cipher: 
               Master_SSL_Key: 
        Seconds_Behind_Master: NULL
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 1236
                Last_IO_Error: Got fatal error 1236 from master when reading data from binary log: 'Binary log is not open'
```

读写分离（前提：主从数据库）

```
使用OpenStack私有云平台，创建三台云主机vm1、vm2和vm3，首先使用两台云主机完成MariaDB数据库的主从配置，接着根据提供的数据库中间件Mycat。完成Mycat读写分离数据库的配置安装（逻辑库名称使用“USERDB”，数据库密码使用000000）
yum install -y java* MariaDB-client

解压mycat到/usr/local/
修改conf/
将提供附件schema.xml替换,修改内容
+<?xml version='1.0'?>
<!DOCTYPE mycat:schema SYSTEM "schema.dtd">
<mycat:schema xmlns:mycat="http://io.mycat/">
<!--注释：name=USERDB指的是逻辑数据库，在后面添加一个dataNode="dn1"，dn1上绑定的是真是数据库-->
<schema name="USERDB" checkSQLschema="true" sqlMaxLimit="100"
dataNode="dn1"></schema>
<!--注释：name="dn1"上面与逻辑数据库引用的名称，database="test"真
实数据库名字-->
<dataNode name="dn1" dataHost="localhost1" database="test" />
<dataHost name="localhost1" maxCon="1000" minCon="10" balance="3" dbType="mysql"
dbDriver="native" writeType="0" switchType="1" slaveThreshold="100">
 <heartbeat>select user()</heartbeat>
 <writeHost host="db1" url="192.168.200.12:3306" user="root" password="000000">
 <readHost host="db2" url="192.168.200.13:3306" user="root" password="000000" />
 </writeHost>
</dataHost>
</mycat:schema>

删除server.xml最后并替换自己的密码和名称

启动/bin/mycat start            #记得授予数据库远程访问权限

查看 
mysql -uroot -pAbc@1234 -h 127.0.0.1 -P 8066
show @@datasource
```

![image-20221128195155929](私有云记忆点.assets/image-20221128195155929.png)

```
报错找不到xnode1
修改主机名和解析名一致
```

RabbitMQ集群（三台）

```
修改主机名，写域名解析
hostnamectl set-hostname rabbitmq1
hostnamectl set-hostname rabbitmq2
hostnamectl set-hostname rabbitmq3

cat >> /etc/hosts << EOF
192.168.200.51 rabbitmq1
192.168.200.52 rabbitmq2
192.168.200.53 rabbitmq3
EOF

yum install -y rabbitmq-server
systemctl start rabbitmq-server && systemctl enable rabbitmq-server  && systemctl stop rabbitmq-server
启动服务后在/var/lib/rabbitmq下生成  .erlang.cookie
停止服务
将三台cook换成相同的    wq!强制
重启改动cook的两台

如果传输的话# 传输完后会导致用户与用户组更改 需要改回
chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie

systemctl start rabbitmq-server
rabbitmqctl stop_app
停止后加入集群
rabbitmqctl join _cluster --ram rabbit@rabbitmq1    #一定要加域名解析
启动  rabbitmqctl start_app
查看  rabbitmqctl cluster_status


开启rabbitmq图形化面板  #端口15672

rabbitmq-plugins enable rabbitmq_management

systemctl restart rabbitmq-server
```

手动迁移云主机

```
1. 迁移虚拟机目录
/var/lib/nova/instances/中对应的虚拟机目录
2. 修改数据库文件
nova库中instances表对应虚拟机的host和node字段改成另一台主机名  
select * from instance \G;    查看uuid    
update instance set host='controller',node='controller' where uuid='XXX';
3. 重启opensatck-nova-compute
```

调整云主机大小配置

```
使用OpenStack私有云平台，使用centos7.9镜像，flavor使用1vcpu/2G内存/40G硬盘，创建云主机cscc_vm，假设在使用过程中，发现该云主机配置太低，需要调整，请修改相应配置，将dashboard界面上的云主机调整实例大小可以使用，将该云主机实例大小调整为2vcpu/4G内存/40G硬盘

#两个节点都改一下
vi /etc/nova/nova.conf 
[DEFAULT]
allow_resize_to_same_host = True 
scheduler_default_filters = RetryFilter,AvailabilityZoneFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter

systemctl restart openstack-nova*


openstack flavor create --disk 40 --ram 4096 --vcpus 2 centos1
openstack server resize --flavor centos1 --wait cscc_vm
```

将云主机打快照

```
将云主机打快照并保存到controller节点/root/cloudsave目录下，保存名字为csccvm.qcow2。最后使用qemu-img相关命令，将镜像的campat版本修改为0.10（该操作是为了适配某些低版本的云平台）
openstack server image create cirros-test --name cirrosimages

openstack image save --file /root/cloudsave/csccvm.qcow2   csccvm.qcow2

# cd /var/lib/glance/images/
qemu-img info XXXXXX    查看镜像信息

qemu-img amend -f qcow2 -o compat=0.10 XXXX
```

磁盘阵列raid5

```
在OpenStack私有云平台，创建一台云主机，并创建一个40G大小的cinder块存储，将块存储连接到云主机，然后在云主机上对云硬盘进行操作。要求分出2个大小为10G的分区，使用这2个分区，创建名为/dev/md0、raid级别为0的磁盘阵列，最后将md0格式化为ext4格式并挂载至/mnt目录

cinder type-create lvm
cinder create --volume-type lvm --name block 40
openstack server add volume VM1 block

安装mdadm
yum install -y mdadm
分区
mdadm -C /dev/md1 -v -l 5 -n3 -x1 /dev/sdb{1,2,3,5}
-C v：创建设备，并显示信息
-l 5：RAID的等级为RAID 5
-n 3：创建RAID的设备为3块
四种模式分别是：创建（-C），装配（-A），监控（-F），管理（-f,-r,-a）

查看 mdadm -D /dev/md1     #进度要到100%，不然0分
```

![image-20221107175235469](D:\疯狂内卷文件\云计算省赛准备\省赛记忆手册github\Provincial-competition-memory-handbook\私有云\私有云记忆点.assets\image-20221107175235469.png)

![image-20221107175757623](D:\疯狂内卷文件\云计算省赛准备\省赛记忆手册github\Provincial-competition-memory-handbook\私有云\私有云记忆点.assets\image-20221107175757623.png)

使用提供的OpenStack私有云平台，修改普通用户权限，使普通用户不能对镜像进行创建和删除操作

```
vi /etc/glance/policy.json
"add_image": " role:admin",
"delete_image": "role:admin",
```

数据库备份mysqldump

```
创建一台云主机，编写脚本，要求可以完成数据库的定期备份，并把数据库备份文件存放在/opt目录下
#!/bin/bash 
mysqldump -uroot -p000000 --all-databases > /opt/mysql.sql

定时命令
crontab -e
50 03 * * * cd /root/;sh mysql_backup.sh

crontab -l   查看

说明
 mysqldump 
Usage: mysqldump [OPTIONS] database [tables]
OR     mysqldump [OPTIONS] --databases [OPTIONS] DB1 [DB2 DB3...]
OR     mysqldump [OPTIONS] --all-databases [OPTIONS]
For more options, use mysqldump --help
```

调整虚拟机内存

```
使用OpenStack私有云平台，找到virsh中ID为10的云主机（若不存在请自行创建）。在云主机所在的物理节点，进入virsh交互式界面，使用virsh命令，将memory虚拟机的内存调整为5242880KB大小

virsh list --all
virsh shutdown instance-00000001
virsh edit instance-00000001
  <memory unit='KiB'>5242880</memory>
      <currentMemory unit='KiB'>5242880</currentMemory>

virsh start instance-00000001
```

kafka集群（依赖zookeeper集群）

```
zookeeper
安装java
重命名zoo.cfg
添加 server.1 = 192.168.100.10:2888:3888
    server.2 = 192.168.100.20:2888:3888
    server.3 = 192.168.100.30:2888:3888
创建数据文件夹  mkdir /tmp/zookeeper
将id写入myid   固定
echo "1"  >> /tmp/zookeeper/myid

启动服务

kafka集群
修改conf文件修改server.properties文件
broker.id=1
listeners=PLAINEXT://192.168.100.10:9092

启动服务
./kafka-server-start.sh -daemon ../config/server.properties
```

创建卷和逻辑卷

```
vgcreate vg1000 /dev/sdb1 /dev/sdb2  #创建卷组"vg1000"

使用lvcreate命令在卷组"vg1000"上创建一个200MB的逻辑卷。在命令行中输入下面的命令
lvcreate -L 200M vg1000    #创建大小为200M的逻辑卷
```

优化KVM的I/O调度算法

```
使用提供的OpenStack私有云平台，优化KVM的I/O调度算法，将默认的deadline修改为none模式
echo none > /sys/block/vda/queue/scheduler
```

redis一主二从

```
redis主从
修改配置文件
#主
#第一处修改
# bind 127.0.0.1                     //找到bind 127.0.0.1这行并注释掉
#第二处修改
protected-mode yes                   //修改前
protected-mode no                   //修改后，外部网络可以访问
#第三处修改
daemonize no                        //修改前
daemonize yes                       //修改后，开启守护进程
#第四处修改
# requirepass foobared                 //找到该行
requirepass "123456"                   //在下方添加设置访问密码
#第五处修改，设定主库密码与当前库密码同步，保证从库能够提升为主库
masterauth "123456"
#第六处修改，打开AOF持久化支持
appendonly yes

#从
#第一处修改
# bind 127.0.0.1                     //找到bind 127.0.0.1这行并注释掉
#第二处修改
protected-mode yes                   //修改前
protected-mode no                   //修改后，外部网络可以访问
#第三处修改
daemonize no                        //修改前
daemonize yes                       //修改后，开启守护进程
#第四处修改
# requirepass foobared                 //找到该行
requirepass "123456"                   //在下方添加设置访问密码
#第五处修改
# slaveof <masterip> <masterport>       //找到该行
slaveof 192.168.200.21 6379          //在下方添加访问的主节点IP与端口
#第六处修改
# masterauth <master-password>        //找到该行
masterauth "123456"                   //在下方添加访问主节点密码
#第七出修改，打开AOF持久化支持
appendonly yes

查看
redis-cli 
auth 123456
info

```

redis三哨兵

```
cp /etc/redis-sentinel.conf redis-sentinel.conf 
vi redis-sentinel.conf
protected-mode no
daemonize yes
sentinel monitor mymaster 192.168.200.51 6379 2
sentinel auth-pass mymaster 123456

启动   #先主再从
redis-sentinel redis-sentinel.conf 
```

云主机起不来

```
因为当前环境为本地PC环境的VMWare Workstation软件启动的虚拟机，所以在此openstack平台启动云主机，需要对openstack平台配置文件进行修改，修改nova服务配置文件，设置参数“virt_type=qemu”。

crudini --set /etc/nova/nova.conf libvirt virt_type  qemu
systemctl restart openstack-nova-compute
```

Vpn

```
在admin租户创建vpn连接，--peer-address为demo租户的路由route2网关地址100.0.0.22
[root@controller ~]# source /etc/keystone/admin-openrc.sh
[root@controller ~]# openstack vpn ike policy create ikepolicy1
[root@controller ~]# openstack vpn ipsec policy create ipsecpolicy1
[root@controller ~]# openstack vpn service create --router route1 --subnet net1 vpn1
[root@controller ~]# openstack vpn ipsec site connection create vpnconnectiona --vpnservice vpn1 --ikepolicy ikepolicy1  --ipsecpolicy ipsecpolicy1 --peer-address 100.0.0.22 --peer-id 100.0.0.22 --peer-cidr 100.0.2.0/24 --psk secret
```

### 私有云运维开发任务

Ansible服务部署：部署MariaDB集群

```bash
[root@controller example]# tree ../example
../example
├── cscc_install.yaml
├── group_vars
│   └── all
└── roles
    └── mariadb
        ├── defaults
        │   └── main.yml
        ├── files
        │   └── gpmall.repo
        ├── handlers
        │   └── main.yml
        ├── meta
        │   └── main.yml
        ├── README.md
        ├── tasks
        │   └── main.yml
        ├── templates
        │   ├── hosts.j2
        │   └── server.cnf.j2
        ├── tests
        │   ├── inventory
        │   └── test.yml
        └── vars
            └── main.yml


[root@controller example]# cat roles/mariadb/tasks/main.yml 
---
- name: mv repo
  shell: mv /etc/yum.repos.d/* /mnt/
- name: copy repo
  copy: src=gpmall.repo dest=/etc/yum.repos.d/
- name: config hosts
  template: src=hosts.j2 dest=/etc/hosts
- name: install mariadb-server
  yum: name=mariadb-server state=present
- name: start mariadb
  shell: systemctl start mariadb
- name: enable  mariadb
  shell: systemctl enable mariadb 
- name: install expect
  yum: name=expect state=present 
- name: init mariadb
  shell: mysqladmin -u root password '123456'
- name: config server.cnf
  template: src=server.cnf.j2 dest=/etc/my.cnf.d/server.cnf
- name: grant privileges
  shell: mysql -uroot -p123456 -e "grant all privileges on *.* to root@'%' identified by '123456';"
- name: stop mariadb
  shell: systemctl stop mariadb 
- name: new galera
  shell: galera_new_cluster 
  when: ansible_fqdn=="node1"
- name: start mariadb 1
  shell: systemctl restart mariadb 
  when: ansible_fqdn=="node2"
- name: start mariadb 2
  shell: systemctl restart mariadb 
  when: ansible_fqdn=="node3"





[root@controller example]# cat roles/mariadb/templates/server.cnf.j2 
[galera]
# Mandatory settings
wsrep_on=ON
wsrep_provider=/usr/lib64/galera/libgalera_smm.so 
wsrep_cluster_address="gcomm://{{HOSTIP1}},{{HOSTIP2}},{{HOSTIP3}}"
binlog_format=row
default_storage_engine=InnoDB
innodb_autoinc_lock_mode=2

{% if ansible_fqdn=="node1" %}
bind-address={{HOSTIP1}}
{% elif ansible_fqdn=="node2" %}
bind-address={{HOSTIP2}}
{% else %}
bind-address={{HOSTIP3}}
{% endif %}
```

