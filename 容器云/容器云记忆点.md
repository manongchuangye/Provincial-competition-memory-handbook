

### 基于Docker Compose编排部署Pig快速开发框架

#### 1、【实操题】容器化部署MariaDB（1分）

在master节点上编写/root/Pig/Dockerfile-mariadb文件构建pig-mariadb:v1.0镜像，具体要求如下：（需要用到的软件包：Pig.tar.gz）

（1）基础镜像：centos:centos7.9.2009；

（2）安装并初始化MariaDB，密码：root；

（3）将pig_codegen.sql、pig_config.sql、pig_job.sql、pig.sql导入数据库；

（4）声明端口：3306；

（5）设置MariaDB开机自启。

完成后构建镜像，并提交master节点的IP地址、用户名和密码到答题框。



#### 2、【实操题】容器化部署Redis（1分）

在master节点上编写/root/Pig/Dockerfile-redis文件构建pig-redis:v1.0镜像，具体要求如下：（需要用到的软件包：Pig.tar.gz）

（1）基础镜像：centos:centos7.9.2009；

（2）安装redis服务；

（3）修改配置文件中的bind 127.0.0.1为bind 0.0.0.0；

（4）设置redis免密，并关闭保护模式；

（5）声明端口：6379；

（6）设置服务开机自启。

完成后构建镜像，并提交master节点的IP地址、用户名和密码到答题框。



#### 3、【实操题】容器化部署Pig（1.5分）

在master节点上编写/root/Pig/Dockerfile-nacos文件构建pig-nacos:v1.0镜像，具体要求如下：（需要用到的软件包：Pig.tar.gz）

（1）基础镜像：centos:centos7.9.2009；

（2）安装openjdk服务；

（3）声明端口：3000、4000、8848、9999；

（4）使用提供的jar包启动pig服务并设置开机自启。

完成后构建镜像，并提交master节点的IP地址、用户名和密码到答题框。



#### 4、【实操题】容器化部署前端服务（1分）

在master节点上编写/root/Pig/Dockerfile-nginx文件构建pig-ui:v1.0镜像，具体要求如下：（需要用到的软件包：Pig.tar.gz）

（1）基础镜像：centos:centos7.9.2009；

（2）安装nginx服务；

（3）将dist拷贝到/data目录下，并将pig-ui.conf拷贝到/etc/nginx/conf.d/目录下；

（4）设置nginx服务开机自启。

完成后构建镜像，并提交master节点的IP地址、用户名和密码到答题框。



#### 5、【实操题】编排部署Pig快发开发平台（2.5分）

在master节点上编写/root/Pig/docker-compose.yaml文件，具体要求如下：

（1）容器1名称：pig-mysql；镜像：pig-mariadb:v1.0；端口映射：3306:3306；

（2）容器2名称： pig-redis；镜像：pig-redis:v1.0；端口映射：6379:6379；

（3）容器3名称：pig-service；镜像：pig-server:v1.0；端口映射：8848:8848、9999:9999；

（4）容器4名称：pig-ui；镜像：pig-server:v1.0；端口映射：80:8888。

完成后编排部署Pig快速开发平台。然后提交master节点的IP地址、用户名和密码到答题框。



### 基于Kubernetes + Jenkins + GitLab + Harbor构建持续集成

#### 【实操题】安装Jenkins环境（1.5分）

在Kubernetes集群default命名空间下完成Jenkins的部署，使用镜像jenkins/jenkins:latest，Deployment和Service名称均为jenkins，要求以NodePort方式将Jenkins的8080端口对外暴露为30880，并完成离线插件的安装。部署完成后设置Jenkins用户名为jenkins；密码为000000，并在授权策略中配置“任何用户可以做任何事(没有任何限制)”。

完成后提交master节点的用户名、密码和IP地址到答题框。（需要用到的软件包路http://<IP>/Jenkins.tar.gz）

```yaml
[root@k8s-master-node1 Jenkins]# cat jenkins.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: jenkins
  name: jenkins
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jenkins
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: jenkins
    spec:
      containers:
      - image: jenkins/jenkins:latest
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsUser: 0
        name: jenkins
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: jenkins
          mountPath: /var/jenkins_home
        - name: jenkins2
          mountPath: /usr/bin/docker
        - name: jenkins3
          mountPath: /var/run/docker.sock
        - name: jenkins4
          mountPath: /root/.kube
        - name: jenkins5
          mountPath: /usr/local/bin/kubectl
      volumes:
      - name: jenkins
        hostPath:
          path: /etc/jenkins_home
      - name: jenkins2
        hostPath:
          path: /usr/bin/docker
      - name: jenkins3
        hostPath:
          path: /var/run/docker.sock
      - name: jenkins4
        hostPath:
          path: /root/.kube
      - name: jenkins5
        hostPath:
          path: /usr/bin/kubectl
---
apiVersion: v1
kind: Service
metadata:
 name: jenkins
spec:
 type: NodePort
 ports:
 - port: 8080
   targetPort: 8080
   nodePort: 30880
 selector:
  app: jenkins
```

#### 【实操题】安装GitLab环境（1.5分)

在Kubenetes集群default命名空间下完成GiLab的部署，使用镜像gitabl/gitab-ce:latest,Deployment和Service名称均为gitlab，要求在eployment资源清单文件中设置GitL ab的root用户密码为admin123，并以NodePort方式将GitLab的80端口对外暴露为30888。部署完成后新建开项目springcloud，并将springcloud文件夹中的代码上传到该项目。
完成后提交master节点的用户名、密码和IP地址到答题框。(需要用到的软件包路径http:l<lP>/Jenkins.tar.gz)

```yaml
[root@k8s-master-node1 Jenkins]# cat gitlab.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: gitlab
  name: gitlab
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gitlab
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: gitlab
    spec:
      containers:
      - image: gitlab/gitlab-ce:latest
        name: gitlab-ce
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
        env: 
        - name: DB_USER
          value: root
        - name: DB_PASS
          value: admin123
        resources: {}
status: {}
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: gitlab
  name: gitlab
spec:
  ports:
  - name: gitlab-svc
    port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30888
  selector:
    app: gitlab
  type: NodePort
status:
  loadBalancer: {}
```

#### 【实操题】配置Jenkins连接GitLab (1分)

配置Jenkins连接GitLab，在GlLab中生成名为jenkins的"Access Tokens"，在Jenkins中配置GitLab并测试其连通性。完成后提交master节点的用户名、密码和IP地址到答题框。(需要用到的软件包路径http:/<IP>/Jenkins.tar.gz)

#### 【实操题】配置Jenkins连接maven (1分)

在Jenkins容器中完成Maven的安装，并在Jenkins全局工具配置中新增Maven。
完成后提交master节点的用户名、密码和IP地址到答题框。(需要用到的软件包路径http:/<lP>/Jenkins.tar.gz)

#### 【实操题】构建CI/CD (3分)??1

在Jenkins中新建一个流水线任务springcloud，基于GltLab中的springcloud项目，编写流水线脚本，触发构建，完成项目代码构建
并构建
docker镜像推送到Harbor仓库的springcloud项目，基于新构建的镜像完成config和gateway服务自动发布到Kubermetes集群springcloud命名空间下。
完成后提交master节点的用户名、密码和IP地址到答题框。(需要用到的软件包路径http:;l<lP>/Jenkins.tar.gz)



### 容器云运维** //10

#### 参数调优--修改Pod数量限制(1分)

```
Kubernetes默认每个节点只能启动110个Pod，由于业务需要，需要在所有节点上运行大量资源消耗非常小的Pod，请将每个节点默认限制的Pod数量改为200。完成后提交master节点的用户名、密码和IP到答题框。

一般来说，我们只需要在 kubelet 启动命令中增加 --max-pods 参数，然后，重启 kubelet 服务，就生效。
重启 kubelet，不影响现有运行中的容器，不会造成容器重启。

1.登录 Node 节点，查看kubelet启动文件路径
 systemctl status kubelet
2.查看 systemd 启动文件
可以看到，Environment 文件是引用 /etc/kubernetes/kubelet，至此，我们只需要在/etc/kubernetes/kubelet 中增加 max-pods 的变量，然后在/usr/lib/systemd/system/kubelet.service 中，将变量添加到启动参数后即可。
3.修改 /etc/kubernetes/kubelet
强烈建议，在修改 Pod 数量的时候，如果是缩减（当前运行70个Pod，所见到20个），强烈建议先把 Node 上的 Pod 驱散，等驱散完了在重启，最后在激活Node。
[root@VM-2-8-centos ~]# vim /etc/kubernetes/kubelet
# 增加
MAX_PODS="--max-pods=61"
4.修改 /usr/lib/systemd/system/kubelet.service
必须在启动命令后加入该变量，才可以使刚才定义的 --max-pods=61 生效。
5.重启 kubelet
[root@VM-2-8-centos ~]# systemctl  daemon-reload
[root@VM-2-8-centos ~]# systemctl restart kubelet
```

#### 参数调优--修改NodePort端口范围（1分）

Kubernetes以NodePort方式暴露服务默认的端口范围为30000-32767，请将NodePort的端口范围修改为20000-65535。完成后提交master节点的用户名、密码和IP到答题框。

```
[root@k8s-master-node1 Jenkins]# vi /etc/kubernetes/manifests/kube-apiserver.yaml 
 resources:
      requests:
    ... ...
    - --service-node-port-range=20000-49999
```

#### RBAC管理--创建角色并并绑定（1分）

```yaml
创建一个名为deployment-clusterrole的clusterrole，并且对该clusterrole只绑定对Deployment、Daemonset、Statefulset的创建权限。在指定命名空间exam创建一个名为exam-user的serviceaccount，并且将上一步创建clusterrole和该serviceaccount绑定。

完成后提交master节点的用户名、密码和IP地址到答题框。
[root@k8s-master-node1 ~]# cat deployment-clusterrole.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: null
  name: deployment-clusterrole
rules:
- apiGroups:
  - apps
  resources:
  - deployments
  - daemonsets
  - statefulsets
  verbs:
  - create

[root@k8s-master-node1 ~]# cat serviceaccount.yaml 
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: exam
spec: {}
status: {}
---
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: null
  name: exam-user
  namespace: exam
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  creationTimestamp: null
  name: deployment-rolebinding
  namespace: exam
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: deployment-clusterrole
subjects:
- kind: ServiceAccount
  name: exam-user
  namespace: exam
```

### 【实操题】服务部署 （4分）

在Kubernetes集群default命名空间下完成ownCloud云盘系统的部署。启动一个名为owncloud的Deployment，包含一个Pod，Pod内包含两个容器owncloud（镜像：owncloud:latest）和mysql（mysql:5.6）。为该Deployment创建一个名为owncloud-svc的Service，以NodePort方式将容器的80端口对外暴露为30003。

完成后提交master节点的用户名、密码和IP地址到答题框。

```
[root@k8s-master-node1 ~]# cat owncloud.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: owncloud-mysql
  name: owncloud-mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: owncloud-mysql
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: owncloud-mysql
    spec:
      containers:
      - image: mysql:5.6
        imagePullPolicy: IfNotPresent
        name: mysql
        ports:
        - containerPort: 3306

      - image: owncloud:latest
        name: owncloud
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80

        resources: {}
status: {}
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: owncloud-svc
  name: owncloud-svc
spec:
  ports:
  - name: owncloud-svc
    port: 80
    protocol: TCP
    targetPort: 80 
    nodePort: 30003
  selector:
    app: owncloud-mysql
  type: NodePort
status:
  loadBalancer: {}
```

### 【实操题】资源配额管理（2分）

创建命名空间quota-example，在master节点/root目录下编写YAML文件quota.yaml，具体要求如下：

（1）ResourceQuota名称：compute-resources；

（2）命名空间：quota-example；

（3）命名空间内所有Pod数量不超过4；

（4）命名空间内所有容器内存申请总和不得超过1G；

（5）命名空间内所有内存限制不得超过2G；

（6）命名空间内所有容器申请的CPU不得超过1；

（7）命名空间内所有容器限制的CPU不得超过2。

完成后使用该YAML文件创建ResourceQuota，并提交master节点的用户名、密码和IP到答题框。

```yaml
[root@k8s-master-node1 ~]# cat quota.yaml 
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: quota-example
spec: {}
status: {}
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
  namespace: quota-example
spec:
  hard:
    pods: 4
    requests.memory: 1Gi
    limits.memory: 2Gi
    requests.cpu: 1
    limits.cpu: 2
```

#### 【实操题】Pod管理--创建多容器Pod（1分）

在default命名空间下创建一个名为multus的Pod，要求该Pod内运行四个容器：nginx（镜像nginx:latest）、redis（镜像redis:latest）、memcached（镜像memcached）和consul（consul:latest）。（http://10.24.1.46/multus-pod.tar）

完成后提交master节点的IP地址、用户名和密码到答题框。

```

```















```
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://pbworxbt.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
```

### docker-compose编排部署Pig微服务快速开发框架

#### 容器化部署MariaDB

##### （1）编写Dockerfile

编写init.sh脚本：

```bash
[root@k8s-worker-node1 ~]# cd Pig/
[root@k8s-worker-node1 Pig]# cat mysql_init.sh
#!/bin/bash
mysql_install_db --user=root
mysqld_safe --user=root &
sleep 8
mysqladmin -u root password 'root'
mysql -uroot -proot -e "grant all on *.* to 'root'@'%' identified by 'root'; flush privileges;"
mysql -uroot -proot -e " source /opt/pig.sql;source /opt/pig_codegen.sql;source /opt/pig_config.sql; source /opt/pig_job.sql;"
```

编写yum源：

```bash
[root@k8s-worker-node1 Pig]# cat local.repo
[pig]
name=pig
baseurl=file:///root/yum
gpgcheck=0
enabled=1
```

编写Dockerfile文件：

```
[root@k8s-worker-node1 Pig]# cat Dockerfile-mariadb
FROM centos:centos7.9.2009
MAINTAINER Chinaskills
RUN rm -rf /etc/yum.repos.d/*
COPY local.repo /etc/yum.repos.d/
COPY yum /root/yum
ENV LC_ALL en_US.UTF-8
RUN yum -y install mariadb-server
COPY mysql /opt/
COPY mysql_init.sh /opt/
RUN bash /opt/mysql_init.sh
EXPOSE 3306
CMD ["mysqld_safe","--user=root"]
```

##### （2）构建镜像

构建镜像：

```bash
[root@k8s-worker-node1 Pig]# docker build -t pig-mysql:v1.0 -f Dockerfile-mariadb .
Sending build context to Docker daemon  890.9MB
Step 1/12 : FROM centos:centos7.9.2009
---> eeb6ee3f44bd
Step 2/12 : MAINTAINER Chinaskills
---> Using cache
---> 815a4a5f2242
Step 3/12 : RUN rm -rf /etc/yum.repos.d/*
---> Using cache
---> 6afa0315cb5b
Step 4/12 : COPY local.repo /etc/yum.repos.d/
---> Using cache
---> 4f07e082cc00
Step 5/12 : COPY yum /root/yum
---> Using cache
---> 7042f9e7f455
Step 6/12 : ENV LC_ALL en_US.UTF-8
---> Using cache
---> df0aa8985d61
Step 7/12 : RUN yum -y install mariadb-server
---> Using cache
---> 9ad09d62d373
Step 8/12 : COPY mysql /opt/
---> Using cache
---> 75adb0e3bbb0
Step 9/12 : COPY mysql_init.sh /opt/
---> Using cache
---> 3cc10e8ca0cc
Step 10/12 : RUN bash /opt/mysql_init.sh
---> Using cache
---> f7fe9f822cc3
Step 11/12 : EXPOSE 3306
---> Using cache
---> 70f2274acbeb
Step 12/12 : CMD ["mysqld_safe","--user=root"]
---> Using cache
---> f088fb18dedf
Successfully built f088fb18dedf
Successfully tagged pig-mysql:v1.0
```



mysql_init.sh

```sh
#!/bin/bash
sleep 20
nohup java -jar /root/pig-register.jar  $JAVA_OPTS  >/dev/null 2>&1 &
sleep 20
nohup java -jar /root/pig-gateway.jar  $JAVA_OPTS >/dev/null 2>&1 &
sleep 20
nohup java -jar /root/pig-auth.jar  $JAVA_OPTS >/dev/null 2>&1 &
sleep 20
nohup java -jar /root/pig-upms-biz.jar  $JAVA_OPTS >/dev/null 2>&1
 $JAVA_OPTS是什么意思   $JAVA_OPTS 一定要写
```

docker-compose容器编排

```yaml
在master节点上编写/root/wordpress/docker-compose.yaml文件
（1）容器名称：wordpress；镜像：wordpress:latest；端口映射：82:80；
（2）容器名称：mysql；镜像：mysql:5.6； 
（3）MySQL root用户密码：123456；
（4）创建数据库wordpress。

version: '3'
services:
  wordpress:
    container_name: wordpress
    image: wordpress:latest
    port:
      - 82:80
    depends_on:
      - mysql
    environment:
      WORDPRESS_DB_HOST: mysql
      WORDPRESS_DB_USER: wordpress
      WORDPRESS_DB_PASSWORD: wordpress
      WORDPRESS_DB_NAME: wordpress
  mysql:
    image: mysql:5.6
    container_name: mysql
    environmrnt:
      MYSQL_ROOT_PASSWORD: 123456
      MYSQL_USER: wordpress
      MYSQL_PASSWORD: 123456
      MYSQL_DATABASE: wordpress
```

```yaml
ownCloud
在master节点上编写/root/owncloud/docker-compose.yaml文件，具体要求如下：
（1）容器1名称：owncloud；镜像：owncloud：latest；
（2）挂载路径：/data/db/owncloud:/var/www/html/data；
（3）owncloud端口映射：5679:80；
（4）容器2名称owncloud-db；镜像：mysql：5.6；
（5）数据库密码：123456。

version: '3'
services:
  owncloud:
    image: owncloud:latest
    container_name: owncloud
    volumes:
      - /data/db/owncloud:/var/www/html/data
    links:
      - mysql:mysql
    ports:
      - "5679:80"
    restart: always
  owncloud-db:
    restart: always
    container_name: owncloud-db
    image: mysql:5.6
    environment:
      MYSQL_ROOT_PASSWORD: "123456"
      MYSQL_DATABASE: owncloud
    ports:
      - "3306:3306"
```

```yaml
LNMP

version: "3"
services:
  mysql:
    hostname: mysql
    restart: always
    container_name: lnmp-mysql
    image: mysql:5.6
    ports:
      - "3306:3306"
    volumes:
      - mysql-config:/etc/mysql
      - mysql-log:/var/log/mysql
      - mysql-data:/var/lib/mysql
    environment:
      MSYQL_ROOT_PASSWORD: root
      MYSQL_USER: user
      MSYQL_PASSWORD: user123
  nginx:
    container_name: lnmp-nginx
    image: nginx
    restart: always
    ports:
      - "83:80"
      - "443:443"
    links:
      - php:php
    volumes:
      - nginx-config:/etc/nginx
      - nginx-log:/var/log/nginx
      - nginx-html:/usr/share/nginx/html
    hostname: nginx
  php:
    image: php:7.2-fpm
    container_name: lnmp-php
    ports:
      - "9000:9000"
    hostname: php
    restart: always
    links:
      - mysql:msyql
    volumes:
      - php-config:/usr/local/etc
      - nginx-html:/var/www/html
      
volumes:
  nginx-config:
  nginx-log:
  nginx-html:
  php-config:
  mysql-config:
  mysql-log:
  mysql-data:
```

商城构建

```

```

创建pod

```
使用 nginx 镜像在 default 命名空间下创建一个名为 exam 的 Pod，并为该 Pod 设置环境变量 exam，其值为 2022
kubectl run exam --env="exam=2022" --image=nginx:latest  --namespace=default  --image-pull-policy='IfNotPresent'
```

污点容忍

```
kubectl taint -h 里有提示

Node - taint > 排斥 Pod
Pod  - tolerations -> 我是真爱，我能容忍污点 -> Node
污点有强制和尽量两种，前者完全排斥，后者尽可能排斥，另外某些污点可以将已经在这台节点上部署的 Pod 逐出，这个过程称为 effect

添加污点格式：
kubectl taint node instance-2 key1=value1:NoSchedule

移除污点：
kubectl taint node instance-2 key1=value1:NoSchedule-

更新污点或覆盖：
kubectl taint node [node] key=value:[effect] --overwrite=true


容忍
kubectl explain pod.spec.tolerations


YAML 示例：

tolerations:
- key: "key1"
  operator: "Exists"
  effect: "NoSchedule"
此 Pod 能够容忍带有 key1 标签的污点，且无论是什么值
```

ETCD

![](容器云记忆点.assets\c3bee037a0bf46b5a1da0f7c18082d26.png)

```
 Kubernetes 使用 ETCD 来存储集群的实时运行数据，为防止服务器宕机导致 Kubernetes 集群数据丢失，请将 Kubernetes 集群数据备份到/root/etcd.db 中。
 ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
 从etcd容器中获取etcdctl命令
 
 
 
 #查看etcd集群成员列表
ETCDCTL_API=3 /usr/local/bin/etcdctl \
 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
 --cert=/etc/kubernetes/pki/etcd/server.crt \
 --key=/etc/kubernetes/pki/etcd/server.key \
 --endpoints="https://127.0.0.1:2379"  member list

#查看etcd端点状态
ETCDCTL_API=3 /usr/local/bin/etcdctl \
 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
 --cert=/etc/kubernetes/pki/etcd/server.crt \
 --key=/etc/kubernetes/pki/etcd/server.key \
 --endpoints="https://127.0.0.1:2379"  endpoint status   

开始备份etcd数据：
#基础格式
ETCDCTL_API=3 etcdctl \
--cacert="${CACERT}" --cert="${CERT}" --key="${KEY}" \
--endpoints=${ENDPOINTS} \
snapshot save /root/etcd.db


ETCDCTL_API=3 /usr/local/bin/etcdctl \
 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
 --cert=/etc/kubernetes/pki/etcd/server.crt \
 --key=/etc/kubernetes/pki/etcd/server.key \
 --endpoints="https://127.0.0.1:2379" snapshot save /root/etcd.db
```

![image-20221123104903384](容器云记忆点.assets\image-20221123104903384.png)

健康检查（存活探针）

![image-20221110181836468](容器云记忆点.assets\image-20221110181836468.png)

![image-20221110181615655](容器云记忆点.assets\image-20221110181615655.png)

```
在master节点/root目录下编写yaml文件liveness_httpget.yaml，具体要求如下：
（1）Pod名称：liveness-http；
（2）命名空间：default；
（3）镜像：nginx；端口：80；
（4）容器启动时运行命令“echo Healty > /usr/share/nginx/html/healthz”；
（5）httpGet请求的资源路径为/healthz，地址默认为Pod IP，端口使用容器中定义的端口名称HTTP；
（6）启动后延时30秒开始运行检测；
（7）每隔3秒执行一次liveness probe。
# cat liveness_httpget.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: liveness-http
spec:
  containers:
  - name: livenessdemo
    image: nginx:latest
    ports:
    - name: http
      containerPort: 80
    lifecycle:
      postStart:
        exec:
          command:
          - /bin/sh
          - -c
          - 'echo Healty > /usr/share/nginx/html/healthz'
    livenessProbe:
      httpGet:
        path: /healthz
        scheme: HTTP
        port: http
      initialDelaySeconds: 30
      periodSeconds: 3
```

创建一个 QoS 类为 Guaranteed 的 Pod

```
对于 QoS 类为 Guaranteed 的 Pod：

Pod 中的每个容器都必须指定内存限制和内存请求
对于 Pod 中的每个容器，内存限制必须等于内存请求
Pod 中的每个容器都必须指定 CPU 限制和 CPU 请求
对于 Pod 中的每个容器，CPU 限制必须等于 CPU 请求

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
  namespace: qos-demo
spec:
  containers:
  - image: nginx:latest
    name: nginx
    ports:
    - containerPort: 80
    resources:
      limits:
        cpu: 100m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 256Mi
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

![image-20221111113658120](容器云记忆点.assets\image-20221111113658120.png)

创建一个 QoS 类为 Burstable 的 Pod

```
如果满足下面条件，将会指定 Pod 的 QoS 类为 Burstable：

Pod 不符合 Guaranteed QoS 类的标准。
Pod 中至少一个容器具有内存或 CPU 请求。
```

创建一个 QoS 类为 BestEffort 的 Pod

```
对于 QoS 类为 BestEffort 的 Pod，Pod 中的容器必须没有设置内存和 CPU 限制或请求
```

创建多容器的 Pod

```
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-4
  namespace: qos-example
spec:
  containers:
 
  - name: qos-demo-4-ctr-1
    image: nginx
    resources:
      requests:
        memory: "200Mi"
 
  - name: qos-demo-4-ctr-2
    image: redis
```

安全管理**--**配置 **Pod** 安全上下文

```
设置pod为特权模式即可
securityContext:
      privileged: true
      
      
 使用 busybox 镜像启动一个名为 context-demo 的 Pod，为该 Pod 配置安全上下文，要求 容器内以用户 1000 和用户组 3000 来运行所有进程，并在启动时执行“sleep 1h”命令。
# cat flask-demo.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: flask-demo
  name: flask-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: flask-demo
  template:
    metadata:
      labels:
        app: flask-demo
    spec:
      securityContext:
        runAsUser: 1000    # 镜像里必须有这个用户UID
        runAsGroup: 3000      
      containers:
      - image: wangjinxiong/flask-demo:root
        name: flask-demo
        command:
        - sleep
        - 1h
        securityContext:
           allowPrivilegeEscalation: false # 不允许提权
        resources: {}
```

CRD

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  # name 必须匹配下面的spec字段：<plural>.<group>
  name: crontabs.stable.example.com
spec:
  # group 名用于 REST API 中的定义：/apis/<group>/<version>
  group: stable.example.com
  # 列出自定义资源的所有 API 版本
  versions:
  - name: v1beta1  # 版本名称，比如 v1、v2beta1 等等
    served: true  # 是否开启通过 REST APIs 访问 `/apis/<group>/<version>/...`
    storage: true # 必须将一个且只有一个版本标记为存储版本
    schema:  # 定义自定义对象的声明规范
      openAPIV3Schema:
        description: Define CronTab YAML Spec
        type: object
        properties:
          spec:
            type: object
            properties:
              cronSpec:
                type: string
              image:
                type: string
              replicas:
                type: integer
  # 定义作用范围：Namespaced（命名空间级别）或者 Cluster（整个集群）
  scope: Namespaced
  names:
    # kind 是 sigular 的一个驼峰形式定义，在资源清单中会使用
    kind: CronTab
    # plural 名字用于 REST API 中的定义：/apis/<group>/<version>/<plural>
    plural: crontabs
    # singular 名称用于 CLI 操作或显示的一个别名
    singular: crontab
    # shortNames 相当于缩写形式
    shortNames:
    - ct


创建自定义资源类型
在 Kubernetes 集群中自定义一种资源类型 Student，API 为 stable.example.com/v1，单数 形式为 student，复数形式为 students，简写为 stu，作用域为命名空间级，然后在 default 命 名空间下创建一个名为 exam 的 Student 对象。
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: students.stable.example.com
spec:
  group: stable.example.com
  versions:
  - name: v1
    storage: true
    served: true
    schema:  
      openAPIV3Schema:
        type: object
  scope: Namespaced
  names:
    kind: Student
    plural: students
    singular: student
    shortNames:
    - stu
```

解析管理**--**添加主机别名到 **Pod**

```
使用 nginx 镜像在 default 命名空间下创建一个名为 nginx 的 Pod，并在 Pod 的/etc/hosts
中添加 IP 地址 127.0.0.1 与 chinaskills 的解析。

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  hostAliases:
  - ip: "127.0.0.1"
    hostnames:
    - "chinaskill"
  containers:
  - image: nginx:latest
    name: nginx
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


[root@master ~]# kubectl exec  nginx -- cat /etc/hosts
# Kubernetes-managed hosts file.
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
fe00::0 ip6-mcastprefix
fe00::1 ip6-allnodes
fe00::2 ip6-allrouters
10.244.0.13     nginx

# Entries added by HostAliases.
127.0.0.1       chinaskill
```

HPA管理

```bash
默认情况下 HPA 是无法调整伸缩灵敏度的，但不同的业务场景对伸缩灵敏度的要求不一样。要求在 default 命名空间下使用 nginx 镜像创建一个名为 web 的 deployment，自定义 HPA 的伸缩灵敏度，为该 deployment 创建一个名为 web 的 HPA，扩容时立即新增当前 9 倍 数量的副本数，时间窗口为 5s，伸缩范围为 1--1000。例如一开始只有 1 个 Pod，当 CPU 使 用率超过 80%时，Pod 数量变化趋势为：1  → 10 →  100 → 1000。


修改yaml文件
kubectl edit deployments nginx

kubectl autoscale deployment nginx --min=1 --max=100 --cpu-percent=80

#腾讯参考https://cloud.tencent.com/developer/article/2060687
```

![image-20221120102935219](D:\疯狂内卷文件\云计算省赛准备\省赛记忆手册github\Provincial-competition-memory-handbook\容器云\容器云记忆点.assets\image-20221120102935219.png)

节点亲和性管理--创建硬限制规则的 Pod

```
在  default  命 名 空 间 下 使 用  nginx  镜 像 运 行 一 个  Pod ， 名 称 为  nginx ， 要 求 使 用 requiredDuringSchedulingIgnoredDuringExecution 策略将 Pod 调度到具有“disktype=ssd”标 签的节点。

先给节点打标签
[root@master ~]# kubectl label node localhost.localdomain disktype=ssd
node/localhost.localdomain labeled


apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
  containers:
  - image: nginx:latest
    name: nginx
    ports:
    - containerPort: 80
  dnsPolicy: ClusterFirst
  restartPolicy: Always
```

![image-20221111192822241](容器云记忆点.assets\image-20221111192822241.png)

创建pv、pvc

```
创建一个名称为 task-pv-volume 的 hostPath 类型的 PV 卷，其映射本地目录为 /mnt/data，卷空间大小为 10Gi，使用 ReadWriteOnce 访问模式，storageClassName: manual

storageClassName 是给卷进行分类，其字段值是自定义的。

apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```

声明 PVC，即代表要使用多大的卷：

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
```

挂载到 Pod 中：

```
apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
```

```

```

```
创建secret

apiVersion: v1
kind: Secret
metadata:
  name: testsecret-tls
  namespace: default
data:
  tls.crt: base64 编码的 cert
  tls.key: base64 编码的 key
type: kubernetes.io/tls
```

```

```

command和args执行的命令会在logs中记录

```
apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: configmap-pod
      image: busybox
      command: ["cat"]
      args: ["/etc/config/config.json"]
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: my-config3
        
root@master:~# kubectl logs configmap-pod
{
    "name": "痴者工良",
    "url": "k8s.whuanle.cn"
}
# cat /etc/config/config.json
```

Secret

```
创建已有的Secret
$ echo -n "admin" | base64
YWRtaW4=
$ echo -n "1f2d1e2e67df" | base64
MWYyZDFlMmU2N2Rm

vi secrets.yml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  password: MWYyZDFlMmU2N2Rm
  username: YWRtaW4=
```

Secret的类型

| 内置类型                              | 用法                                     |
| ------------------------------------- | ---------------------------------------- |
| `Opaque`                              | 用户定义的任意数据                       |
| `kubernetes.io/service-account-token` | 服务账号令牌                             |
| `kubernetes.io/dockercfg`             | `~/.dockercfg` 文件的序列化形式          |
| `kubernetes.io/dockerconfigjson`      | `~/.docker/config.json` 文件的序列化形式 |
| `kubernetes.io/basic-auth`            | 用于基本身份认证的凭据                   |
| `kubernetes.io/ssh-auth`              | 用于 SSH 身份认证的凭据                  |
| `kubernetes.io/tls`                   | 用于 TLS 客户端或者服务器端的数据        |
| `bootstrap.kubernetes.io/token`       | 启动引导令牌数据                         |

volumes

```yaml
引入docker卷
docker volume create {卷名称}
docker run   ...  -v /opt/test:/opt/test 

hostPath(依赖节点上的目录或文件，不同节点无法共享，适合单个pod部署，deployment多pod分布在不同节点)
spec:
  containers:
  - image: nginx:latest
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # 宿主上目录位置
      path: /data
      # 此字段为可选
      type: Directory

emptyDir(空卷，不需要指定宿主机上对应目录，生命周期与pod绑定，容器崩溃不会影响，可在同一pod不同容器中访问)
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: nginx:latest
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
    
git卷(拉取git中的代码，还没有涉及此类的题)
volumns:
- name: erp
  gitRepo: 
    repository: https://github.com/whuanle/CZGL.AOP
    revision: main
    directory: .
    
ConfigMap(存储非机密性的数据到键值对，信息会被存储到etcd中，任何节点都可访问)
kubectl create configmap <map-name> <data-source>
以文件(--from-file)形式创建 ConfigMap,则为 key，文件内容为 value,也可以指定目录
对(--from-literal)形式创建 ConfigMap，可直接创建 key=value(常用)
1.ConfigMap在环境变量中使用，定义 Pod 
（1）只使用一部分（valueFrom）
    env:
     - name: letter
       valueFrom:
         configMapKeyRef:
           name: my-config2
           key: a.txt
（2）全部导入（envFrom）
    env:
    - name: myconfig_b
      envFrom:
      - configMapRef:
          name: my-config2
2.ConfigMag卷(向pod中注入数据，主要给Pod中容器传递配置)
apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: configmap-pod
      image: busybox
      command: ["ls"]
      args: ["/etc/config"]
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: my-config
 注：ConfigMap 卷映射到容器，如果使用的是 mountPath 挂载到目录，则 ConfigMap 更新内容时，容器中的配置文件内容也会被更新，但需要使用一个新的目录挂载
多键值对
--from-file 等导入 ConfigMap，并不能同时导入多个值，要使用JSON、YAML文件
--from-env-file 将 a.txt 中的多行键值对拆分出来，只能出现一个
kubectl create configmap my-config3 --from-env-file=a.txt

Secret 卷（传递pod敏感信息）
1.通过证书创建 Secret(TLS)
kubectl create secret tls tls-secret --cert=1_k1.whuanle.cn_bundle.crt --key=2_k1.whuanle.cn.key
2.基本身份认证 Secret(basic-auth)存储账号密码，spec.data 中必须包含 username 和 password 两个字段
apiVersion: v1
kind: Secret
metadata:
  name: secret-basic-auth
type: kubernetes.io/basic-auth
stringData:
  username: admin
  password: t0p-Secret

root@master:~# kubectl get secret secret-basic-auth  -o yaml
apiVersion: v1
data:
  password: dDBwLVNlY3JldA==
  username: YWRtaW4=
kind: Secret
会被使用 base64 编码
3.SSH 身份认证 Secret（存储一段 ssh 密钥）
apiVersion: v1
kind: Secret
metadata:
  name: secret-ssh-auth
type: kubernetes.io/ssh-auth
data:
  ssh-privatekey: |
          MIIEpQIBAAKCAQEAulqb/Y ...
使用Secret（环境变量和文件两种形式）
1.文件
apiVersion: v1
kind: Pod
metadata:
  name: secret1
spec:
  containers:
  - name: secret1
    image: nginx
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: secret-basic-auth
2.环境变量
apiVersion: v1
kind: Pod
metadata:
  name: mysecret2
spec:
  containers:
  - name: mysecret
    image: nginx
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: secret-basic-auth
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: secret-basic-auth
            key: password
  restartPolicy: Never
 
同时使用ConfigMap、Secret
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: mydb
  name: mydb
spec:
  containers:
  - image: mysql:5.7
    name: mydb
    envFrom:
    - configMapRef:
        name: db-config
    - secretRef:
        name: db-secret
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

NFS卷（保证每个节点上访问到的文件内容、状态都是一致的、实时的）
echo "/nfs-share *(rw,no_root_squash,sync)" >> /etc/exports
exportfs -r   #生效
exportfs      #检查
systemctl enable --now rpcbind
systemctl enable --now nfs
showmount  -e hostname 
mount {部署了nfs服务器的ip}:/nfs-share /mnt
-------------------------------------------------------------------
挂载到pod
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
    - name: nginx
      image: nginx:latest
      volumeMounts:
        - name: mynfs
          mountPath: /mnt/mynfs
  volumes:
    - name: mynfs
      nfs:
        server: 10.170.0.2
        path: /nfs-share
#查看
kubectl exec nginx -- ls /mnt/mynfs

---------------------------------------------------------
持久卷
创建远程存储空间（创建云硬盘）
#创建一个 /data/volumns 目录，这些目录存储空间用于创建卷
mkdir /data
mkdir /data/volumns
mkdir /data/volumns/1
mkdir /data/volumns/2
mkdir /data/volumns/3
mkdir /data/volumns/4
mkdir /data/volumns/5
echo "/data/volumns/1 *(rw,no_root_squash,sync)" >> /etc/exports
echo "/data/volumns/2 *(rw,no_root_squash,sync)" >> /etc/exports
echo "/data/volumns/3 *(rw,no_root_squash,sync)" >> /etc/exports
echo "/data/volumns/4 *(rw,no_root_squash,sync)" >> /etc/exports
echo "/data/volumns/5 *(rw,no_root_squash,sync)" >> /etc/exports
exportfs -r

创建卷（创建持久卷 PersistentVolume (PV)）
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0001
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /data/volumns/1
    server: 10.0.0.4
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0002
spec:
  capacity:
    storage: 2Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /data/volumns/2
    server: 10.0.0.4
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 4Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /data/volumns/3
    server: 10.0.0.4
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0004
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /data/volumns/4
    server: 10.0.0.4
#这里暂时不创建 pv 5
kubectl get pv
```

每个 PV 可以设定三种 ACCESS MODES，即访问模式：

- `ReadWriteOnce`，RWO

  卷可以被一个节点以读写方式挂载，即只允许被一个节点挂载。

- `ReadOnlyMany`，ROX

  卷可以被多个节点以只读方式挂载。

- `ReadWriteMany`，RWX

  卷可以被多个节点以读写方式挂载。

- `ReadWriteOncePod`，RWOP

  卷可以被单个 Pod 以读写方式挂载。即只能有一个节点一个 Pod 挂载访问。

```yaml
创建PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc3
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi

root@master:~# kubectl get pvc
一旦找到符合要求的 PV，则 PVC 变成 Bound 状态，查找发现 PV 中没有刚好的容量，则会使用稍大的 PV 卷

Pod中使用卷
apiVersion: v1
kind: Pod
metadata:
  name: pvcpod
spec:
  containers:
    - name: pvctest
      image: nginx
      volumeMounts:
      - mountPath: "/usr/share/nginx/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: pvc3

其他类型PV
hostPath
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```

节点选择器

```yaml
Pod节点选择器
spec:
  nodeSelector:
    nodeName: nginxnode
```

**SecurityContext**安全上下文（授予权限）

```yaml
设置运行用户和用户组
spec:
  securityContext: # add security context
    runAsUser: 1000
    runAsGroup: 2000

授权管理用户权限
spec:
  containers:
  - image: nginx
    securityContext:
      capabilities:
        add: ["SYS_TIME", "NET_ADMIN"]
```

**ServiceAccount**（服务账户）

```
创建 ServiceAccount
kubectl create sa admin
在 Pod 中使用此 ServiceAccount
spec:
  serviceAccountName: admin
```

资源限制

```
    resources: 
      requests:
        memory: "100Mi"
        cpu: "0.5"
      limits:
        memory: "200Mi"
        cpu: "1"
```

健康检查

```
命令形式的检查：
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
      
HTTP 形式的检查：
  httpGet:
    httpHeaders:
      - name: Accept
        value: application/json

TCP：
      tcpSocket:
        port: 8080
```

**网络**

```
创建带端口的 Pod：
kubectl run nginx --image=nginx --restart=Never --port=80

为一个 Pod 暴露端口：
kubectl expose po nginx --port=80 --type=NodePort

映射成不同的端口：
kubectl expose po nginx --port=80 --target-port=8080 --type=NodePort
```

exam.qcow2转换为docker镜像

```
把qcow2镜像的整个系统转成文件压缩包，再把文件压缩包导入到docker的镜像仓库
把qcow2镜像挂载到目录下，并复制到一个目录下，进行打包
tar -cvzf ~/com.tar.gz  -C  /mnt/com  ./

docker import  my_ubuntu_v3.tar runoob/ubuntu:v4  
```

![image-20221126143239389](容器云记忆点.assets\image-20221126143239389.png)

![image-20221126143548208](容器云记忆点.assets\image-20221126143548208.png)

回滚

```
kubectl rollout history deployment nginx

kubectl rollout undo deployment nginx --to-revision=2
```

资源限制

```sh
kubectl set resources deployment nginx-app -c=nginx --limits=cpu=500m,memory=128Mi
# -c 指定容器
```

#### 容器云应用部署

准备工作

```
vi local.repo
[yumrepo]
name=yumrepo
baseurl=file:///root/yum
gpgcheck=0
enabled=1
```

##### Hyperf

容器化 MariaDB 服务

```dockerfile
编写 Dockerfile 文件构建 hyperf-mariadb:v1.0 镜像，具体要求如下：（需要用到的软件包：
Hyperf.tar.gz）
（1）基础镜像：centos:7.9.2009；
（2）完成 MariaDB 服务的安装；
（3）声明端口：3306；
（4）设置数据库 root 用户的密码为 root；
（5）将提供的数据库文件 hyperf_admin.sql 导入数据库；
（6）设置服务开机自启。
mysql_init.sh
#!/bin/bash
mysql_install_db --user=root   #初始化mysql
mysqld_safe --user=root &      #启动mysql服务 一定要后台启动
sleep 8
mysqladmin -uroot password 'root' #设置root密码
mysql -uroot -proot -e "grant all on *.* to 'root'@'%' identified by 'root';flush privileges;"      #授予远程权限，并重新加载权限
mysql -uroot -proot -e "source XXX"   #加载数据库

Dockerfile-mariadb 
FROM centos:centos7.9.2009
MAINTAINER Chinaskills
RUN rm -rf /etc/yum.repos.d/*
COPY local.repo /etc/yum.repos.d/
COPY yum /root/yum
ENV LC_ALL en_US.UTF-8              #使用locale查看
RUN yum -y install mariadb-server
COPY mysql /opt/                     #把sql文件拷过去
COPY mysql_init.sh /opt/
RUN bash /opt/mysql_init.sh
EXPOSE 3306
CMD ["mysqld_safe","--user=root"]
```

构建镜像

```
docker build -t mysql:v1.0 -f Dockerfile .
```

容器化Redis服务

```dockerfile
编写 Dockerfile 文件构建 hyperf-redis:v1.0 镜像，具体要求如下：（需要用到的软件包：Hyperf.tar.gz）
（1）基础镜像：centos:7.9.2009；
（2）安装 Redis 服务；
（3）关闭保护模式；
（4）声明端口：6379；
（5）设置服务开机自启。

FROM centos:centos7.9.2009
MAINTAINER Chinaskills
RUN rm -rf /etc/yum.repos.d/*
COPY local.repo /etc/yum.repos.d/
COPY yum /root/yum
RUN yum -y install redis
RUN sed -i 's/127.0.0.1/0.0.0.0/g' /etc/redis.conf && \
     sed -i 's/-mode yes/-mode no/g' /etc/redis.conf
EXPOSE 6379
CMD ["/usr/bin/redis-server","/etc/redis.conf"]
```

容器化 Nginx 服务

```dockerfile
编写 Dockerfile 文件构建 hyperf-nginx:v1.0 镜像，具体要求如下：（需要用到的软件包：
Hyperf.tar.gz）
（1）基础镜像：centos:7.9.2009；
（2）安装 nginx 服务；
（3）声明端口：80；
（4）设置服务开机自启。

FROM centos:centos7.9.2009
MAINTAINER Chinaskills
RUN rm -rf /etc/yum.repos.d/*
COPY local.repo /etc/yum.repos.d/
COPY yum /root/yum
RUN yum -y install nginx
COPY nginx/dist /data                  #配置文件里写的此文件夹
ADD nginx/nginx.conf /etc/nginx/conf.d/     #修改配置文件
RUN /bin/bash -c 'echo init ok'
EXPOSE 80
CMD ["nginx","-g","daemon off;"]
```

容器化 Hyperf 服务（https://hyperf.wiki/2.2/#/zh-cn/quick-start/install）

```
编写 Dockerfile 文件构建 hyperf-service:v1.0 镜像，具体要求如下：（需要用到的软件包：
Hyperf.tar.gz）
（1）基础镜像：centos:7.9.2009；
（2）安装 PHP 及扩展；
（3）使用源码编译安装 Swoole

```

编排部署 Hyperf 框架

```yaml
编排部署 Hyperf 框架 编写/root/hyperf/project/docker-compose.yaml 文件，具体要求如下： 
（1）容器 1 名称：hyperf-mysql；镜像：hyperf-mariadb:v1.0；端口映射：3306:3306； 
（2）容器 2 名称：hyperf-redis；镜像：hyperf-redis:v1.0； 
（3）容器 3 名称：hyperf-ui；镜像：hyperf-nginx:v1.0；端口映射：80:8081； 
（4）容器 4 名称：hyperf-service；镜像：hyperf-service:v1.0。

vi docker-compose.yml

version: '3'
services:
  app:
    container_name: hyperf-service
    image:  hyperf-service:v1.0
    command:
      - /bin/sh
      - -c
      - |                 #这一块看不大懂
        cd /data
        cp .env.dev.docker .env
        php -d memory_limit=-1 `which composer` i --no-dev
        rm -rf runtime/*
        php bin/hyperf.php start
    volumes:
      - ./backend:/data
    tty: true
    links:
      - mysql
      - redis
    environment:
      HOST_IP: 127.0.0.1
      HOST_PORT: 9511
  mysql:
    container_name: hyperf-mysql
    image: hyperf-mariadb:v1.0
    ports:
      - 3306:3306
    restart: always
  redis:
    image: hyperf-redis:v1.0
    container_name: hyperf-redis
    restart: always
  nginx:
    image: hyperf-nginx:v1.0
    container_name: hyperf-nginx
    ports:
      - 8081:80
    links:
      - app
    volumes:
      - ./conf.d/:/etc/nginx/conf.d
      - ./docker/log:/var/log/nginx
      - ./frontend:/var/www/frontend
```

